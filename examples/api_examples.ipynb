{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Hub API Examples\n",
    "\n",
    "This notebook demonstrates how to interact with the Evaluation Hub REST API running on `localhost:8000`.\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from uuid import uuid4\n",
    "\n",
    "import requests\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "API_BASE = f\"{BASE_URL}/api/v1\"\n",
    "\n",
    "# Helper function for pretty printing JSON responses\n",
    "def print_json(data):\n",
    "    print(json.dumps(data, indent=2, default=str))\n",
    "\n",
    "# Helper function for API requests\n",
    "def api_request(method: str, endpoint: str, **kwargs) -> requests.Response:\n",
    "    \"\"\"Make an API request with proper error handling.\"\"\"\n",
    "    url = f\"{API_BASE}{endpoint}\"\n",
    "    response = requests.request(method, url, **kwargs)\n",
    "\n",
    "    print(f\"{method.upper()} {url}\")\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "\n",
    "    if response.headers.get('content-type', '').startswith('application/json'):\n",
    "        print(\"Response:\")\n",
    "        print_json(response.json())\n",
    "    else:\n",
    "        print(f\"Response: {response.text}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Check\n",
    "\n",
    "First, let's verify the service is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://localhost:8000/api/v1/health\n",
      "Status: 200\n",
      "Response:\n",
      "{\n",
      "  \"status\": \"healthy\",\n",
      "  \"version\": \"0.1.0\",\n",
      "  \"timestamp\": \"2025-11-09T23:48:08.777698Z\",\n",
      "  \"components\": {\n",
      "    \"mlflow\": {\n",
      "      \"status\": \"healthy\",\n",
      "      \"tracking_uri\": \"http://mlflow:5000\"\n",
      "    }\n",
      "  },\n",
      "  \"uptime_seconds\": 21.269049167633057,\n",
      "  \"active_evaluations\": 0\n",
      "}\n",
      "--------------------------------------------------\n",
      "✅ Service is healthy!\n",
      "Version: 0.1.0\n",
      "Uptime: 21.3 seconds\n"
     ]
    }
   ],
   "source": [
    "response = api_request(\"GET\", \"/health\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    health_data = response.json()\n",
    "    print(\"✅ Service is healthy!\")\n",
    "    print(f\"Version: {health_data['version']}\")\n",
    "    print(f\"Uptime: {health_data['uptime_seconds']:.1f} seconds\")\n",
    "else:\n",
    "    print(\"❌ Service is not responding correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provider Management\n",
    "\n",
    "### List All Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://localhost:8000/api/v1/providers\n",
      "Status: 200\n",
      "Response:\n",
      "{\n",
      "  \"providers\": [\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"provider_name\": \"LM Evaluation Harness\",\n",
      "      \"description\": \"Comprehensive evaluation framework for language models with 167 benchmarks\",\n",
      "      \"provider_type\": \"builtin\",\n",
      "      \"benchmark_count\": 168\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"ragas\",\n",
      "      \"provider_name\": \"RAGAS\",\n",
      "      \"description\": \"Retrieval Augmented Generation Assessment framework\",\n",
      "      \"provider_type\": \"builtin\",\n",
      "      \"benchmark_count\": 4\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"garak\",\n",
      "      \"provider_name\": \"Garak\",\n",
      "      \"description\": \"LLM vulnerability scanner and red-teaming framework\",\n",
      "      \"provider_type\": \"builtin\",\n",
      "      \"benchmark_count\": 4\n",
      "    }\n",
      "  ],\n",
      "  \"total_providers\": 3,\n",
      "  \"total_benchmarks\": 176\n",
      "}\n",
      "--------------------------------------------------\n",
      "Found 3 providers:\n",
      "  - LM Evaluation Harness (lm_evaluation_harness)\n",
      "    Type: builtin\n",
      "    Benchmarks: 168\n",
      "  - RAGAS (ragas)\n",
      "    Type: builtin\n",
      "    Benchmarks: 4\n",
      "  - Garak (garak)\n",
      "    Type: builtin\n",
      "    Benchmarks: 4\n"
     ]
    }
   ],
   "source": [
    "response = api_request(\"GET\", \"/providers\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    providers_data = response.json()\n",
    "    print(f\"Found {providers_data['total_providers']} providers:\")\n",
    "    for provider in providers_data['providers']:\n",
    "        print(f\"  - {provider['provider_name']} ({provider['provider_id']})\")\n",
    "        print(f\"    Type: {provider['provider_type']}\")\n",
    "        print(f\"    Benchmarks: {provider['benchmark_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Specific Provider Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://localhost:8000/api/v1/providers/lm_evaluation_harness\n",
      "Status: 200\n",
      "Response:\n",
      "{\n",
      "  \"provider_id\": \"lm_evaluation_harness\",\n",
      "  \"provider_name\": \"LM Evaluation Harness\",\n",
      "  \"description\": \"Comprehensive evaluation framework for language models with 167 benchmarks\",\n",
      "  \"provider_type\": \"builtin\",\n",
      "  \"benchmarks\": [\n",
      "    {\n",
      "      \"benchmark_id\": \"arc_easy\",\n",
      "      \"name\": \"ARC Easy\",\n",
      "      \"description\": \"ARC Easy evaluation benchmark - AI2 Reasoning Challenge (Easy)\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2376,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_boolq_lev\",\n",
      "      \"name\": \"Aradice Boolq Lev\",\n",
      "      \"description\": \"Aradice Boolq Lev evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3270,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp\",\n",
      "      \"name\": \"Blimp\",\n",
      "      \"description\": \"Blimp evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_anaphor_gender_agreement\",\n",
      "      \"name\": \"Blimp Anaphor Gender Agreement\",\n",
      "      \"description\": \"Blimp Anaphor Gender Agreement evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_animate_subject_trans\",\n",
      "      \"name\": \"Blimp Animate Subject Trans\",\n",
      "      \"description\": \"Blimp Animate Subject Trans evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_coordinate_structure_constraint_complex_left_branch\",\n",
      "      \"name\": \"Blimp Coordinate Structure Constraint Complex Left Branch\",\n",
      "      \"description\": \"Blimp Coordinate Structure Constraint Complex Left Branch evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_determiner_noun_agreement_2\",\n",
      "      \"name\": \"Blimp Determiner Noun Agreement 2\",\n",
      "      \"description\": \"Blimp Determiner Noun Agreement 2 evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_determiner_noun_agreement_with_adj_2\",\n",
      "      \"name\": \"Blimp Determiner Noun Agreement With Adj 2\",\n",
      "      \"description\": \"Blimp Determiner Noun Agreement With Adj 2 evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_determiner_noun_agreement_with_adjective_1\",\n",
      "      \"name\": \"Blimp Determiner Noun Agreement With Adjective 1\",\n",
      "      \"description\": \"Blimp Determiner Noun Agreement With Adjective 1 evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_existential_there_object_raising\",\n",
      "      \"name\": \"Blimp Existential There Object Raising\",\n",
      "      \"description\": \"Blimp Existential There Object Raising evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_existential_there_subject_raising\",\n",
      "      \"name\": \"Blimp Existential There Subject Raising\",\n",
      "      \"description\": \"Blimp Existential There Subject Raising evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_intransitive\",\n",
      "      \"name\": \"Blimp Intransitive\",\n",
      "      \"description\": \"Blimp Intransitive evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_irregular_plural_subject_verb_agreement_1\",\n",
      "      \"name\": \"Blimp Irregular Plural Subject Verb Agreement 1\",\n",
      "      \"description\": \"Blimp Irregular Plural Subject Verb Agreement 1 evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_left_branch_island_simple_question\",\n",
      "      \"name\": \"Blimp Left Branch Island Simple Question\",\n",
      "      \"description\": \"Blimp Left Branch Island Simple Question evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_npi_present_2\",\n",
      "      \"name\": \"Blimp Npi Present 2\",\n",
      "      \"description\": \"Blimp Npi Present 2 evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_passive_1\",\n",
      "      \"name\": \"Blimp Passive 1\",\n",
      "      \"description\": \"Blimp Passive 1 evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_humanities_history_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Humanities History Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Humanities History Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_humanities_philosophy_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Humanities Philosophy Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Humanities Philosophy Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_language_arabic-language_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Language Arabic-Language Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Language Arabic-Language Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_humanities_islamic-studies_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu Middle Humanities Islamic-Studies Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Middle Humanities Islamic-Studies Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_language_arabic-language_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu Middle Language Arabic-Language Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Middle Language Arabic-Language Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_humanities_islamic-studies_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu Na Humanities Islamic-Studies Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Na Humanities Islamic-Studies Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_language_arabic-language-general_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu Na Language Arabic-Language-General Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Na Language Arabic-Language-General Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_other_driving-test_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu Na Other Driving-Test Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Na Other Driving-Test Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_other_general-knowledge_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu Na Other General-Knowledge Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Na Other General-Knowledge Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_humanities_islamic-studies_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Humanities Islamic-Studies Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Humanities Islamic-Studies Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_language_arabic-language_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Language Arabic-Language Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Language Arabic-Language Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_other_management_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu Univ Other Management Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Univ Other Management Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_openbookqa_eng\",\n",
      "      \"name\": \"Aradice Openbookqa Eng\",\n",
      "      \"description\": \"Aradice Openbookqa Eng evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 500,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_boolq\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Boolq\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Boolq evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3270,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_boolq_light\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Boolq Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Boolq Light evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3270,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_mt_boolq_light\",\n",
      "      \"name\": \"Arabic Mt Boolq Light\",\n",
      "      \"description\": \"Arabic Mt Boolq Light evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3270,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"leaderboard_bbh_salient_translation_error_detection\",\n",
      "      \"name\": \"Leaderboard Bbh Salient Translation Error Detection\",\n",
      "      \"description\": \"Leaderboard Bbh Salient Translation Error Detection evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"bleu\",\n",
      "        \"chrf\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"aclue_ancient_chinese_culture\",\n",
      "      \"name\": \"Aclue Ancient Chinese Culture\",\n",
      "      \"description\": \"Aclue Ancient Chinese Culture evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"african_flores\",\n",
      "      \"name\": \"African Flores\",\n",
      "      \"description\": \"African Flores evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"bleu\",\n",
      "        \"chrf\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"afrixnli-irokobench\",\n",
      "      \"name\": \"Afrixnli-Irokobench\",\n",
      "      \"description\": \"Afrixnli-Irokobench evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"afrixnli_amh_prompt_2\",\n",
      "      \"name\": \"Afrixnli Amh Prompt 2\",\n",
      "      \"description\": \"Afrixnli Amh Prompt 2 evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"afrixnli_amh_prompt_5\",\n",
      "      \"name\": \"Afrixnli Amh Prompt 5\",\n",
      "      \"description\": \"Afrixnli Amh Prompt 5 evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"afrixnli_en_direct_ewe\",\n",
      "      \"name\": \"Afrixnli En Direct Ewe\",\n",
      "      \"description\": \"Afrixnli En Direct Ewe evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"afrixnli_en_direct_ibo\",\n",
      "      \"name\": \"Afrixnli En Direct Ibo\",\n",
      "      \"description\": \"Afrixnli En Direct Ibo evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"afrixnli_en_direct_lug\",\n",
      "      \"name\": \"Afrixnli En Direct Lug\",\n",
      "      \"description\": \"Afrixnli En Direct Lug evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"afrixnli_en_direct_sot\",\n",
      "      \"name\": \"Afrixnli En Direct Sot\",\n",
      "      \"description\": \"Afrixnli En Direct Sot evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"afrixnli_en_direct_wol\",\n",
      "      \"name\": \"Afrixnli En Direct Wol\",\n",
      "      \"description\": \"Afrixnli En Direct Wol evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"afrixnli_en_direct_zul\",\n",
      "      \"name\": \"Afrixnli En Direct Zul\",\n",
      "      \"description\": \"Afrixnli En Direct Zul evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_stem_math_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Stem Math Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Stem Math Egy evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_college_mathematics_light\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_high_school_mathematics\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"cmmlu_college_mathematics\",\n",
      "      \"name\": \"Cmmlu College Mathematics\",\n",
      "      \"description\": \"Cmmlu College Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"cmmlu_high_school_mathematics\",\n",
      "      \"name\": \"Cmmlu High School Mathematics\",\n",
      "      \"description\": \"Cmmlu High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_am_high_school_mathematics\",\n",
      "      \"name\": \"Global Mmlu Full Am High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Am High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_ar_high_school_mathematics\",\n",
      "      \"name\": \"Global Mmlu Full Ar High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Ar High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_bn_high_school_mathematics\",\n",
      "      \"name\": \"Global Mmlu Full Bn High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Bn High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_cs_high_school_mathematics\",\n",
      "      \"name\": \"Global Mmlu Full Cs High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Cs High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_de_high_school_mathematics\",\n",
      "      \"name\": \"Global Mmlu Full De High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full De High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_el_high_school_mathematics\",\n",
      "      \"name\": \"Global Mmlu Full El High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full El High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_en_high_school_mathematics\",\n",
      "      \"name\": \"Global Mmlu Full En High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full En High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_es_high_school_mathematics\",\n",
      "      \"name\": \"Global Mmlu Full Es High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Es High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_fa_high_school_mathematics\",\n",
      "      \"name\": \"Global Mmlu Full Fa High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Fa High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_fil_high_school_mathematics\",\n",
      "      \"name\": \"Global Mmlu Full Fil High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Fil High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_piqa_lev\",\n",
      "      \"name\": \"Aradice Piqa Lev\",\n",
      "      \"description\": \"Aradice Piqa Lev evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1838,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_winogrande_eng\",\n",
      "      \"name\": \"Aradice Winogrande Eng\",\n",
      "      \"description\": \"Aradice Winogrande Eng evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1267,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_copa\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Copa\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Copa evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 500,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_copa_light\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Copa Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Copa Light evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 500,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_hellaswag\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Hellaswag\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Hellaswag evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10042,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_hellaswag_light\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Hellaswag Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Hellaswag Light evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10042,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_piqa\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Piqa\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Piqa evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1838,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_piqa_light\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Piqa Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Piqa Light evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1838,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_mt_hellaswag\",\n",
      "      \"name\": \"Arabic Mt Hellaswag\",\n",
      "      \"description\": \"Arabic Mt Hellaswag evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10042,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_mt_piqa\",\n",
      "      \"name\": \"Arabic Mt Piqa\",\n",
      "      \"description\": \"Arabic Mt Piqa evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1838,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"copa_ar\",\n",
      "      \"name\": \"Copa Ar\",\n",
      "      \"description\": \"Copa Ar evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 500,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"copal_id_colloquial\",\n",
      "      \"name\": \"Copal Id Colloquial\",\n",
      "      \"description\": \"Copal Id Colloquial evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 500,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"darijahellaswag\",\n",
      "      \"name\": \"Darijahellaswag\",\n",
      "      \"description\": \"Darijahellaswag evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10042,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"egyhellaswag\",\n",
      "      \"name\": \"Egyhellaswag\",\n",
      "      \"description\": \"Egyhellaswag evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10042,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"hellaswag_ar\",\n",
      "      \"name\": \"Hellaswag Ar\",\n",
      "      \"description\": \"Hellaswag Ar evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10042,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_race\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Race\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Race evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 674,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mt_race_light\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Race Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Race Light evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 674,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_mt_race_light\",\n",
      "      \"name\": \"Arabic Mt Race Light\",\n",
      "      \"description\": \"Arabic Mt Race Light evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 674,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"blimp_drop_argument\",\n",
      "      \"name\": \"Blimp Drop Argument\",\n",
      "      \"description\": \"Blimp Drop Argument evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 9536,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bigbench_gre_reading_comprehension_multiple_choice\",\n",
      "      \"name\": \"Bigbench Gre Reading Comprehension Multiple Choice\",\n",
      "      \"description\": \"Bigbench Gre Reading Comprehension Multiple Choice evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3000,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"eus_reading\",\n",
      "      \"name\": \"Eus Reading\",\n",
      "      \"description\": \"Eus Reading evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3000,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"longbench_qasper\",\n",
      "      \"name\": \"Longbench Qasper\",\n",
      "      \"description\": \"Longbench Qasper evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3000,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"qasper_freeform\",\n",
      "      \"name\": \"Qasper Freeform\",\n",
      "      \"description\": \"Qasper Freeform evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3000,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"ruler_qa_squad\",\n",
      "      \"name\": \"Ruler Qa Squad\",\n",
      "      \"description\": \"Ruler Qa Squad evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3000,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"scrolls_qasper\",\n",
      "      \"name\": \"Scrolls Qasper\",\n",
      "      \"description\": \"Scrolls Qasper evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3000,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_social-science_economics_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Social-Science Economics Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Social-Science Economics Egy evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_social-science_geography_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Social-Science Geography Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Social-Science Geography Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_stem_computer-science_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Stem Computer-Science Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Stem Computer-Science Egy evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_stem_physics_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Stem Physics Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Stem Physics Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_social-science_civics_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu Middle Social-Science Civics Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Middle Social-Science Civics Egy evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_social-science_economics_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu Middle Social-Science Economics Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Middle Social-Science Economics Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_social-science_social-science_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu Middle Social-Science Social-Science Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Middle Social-Science Social-Science Egy evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_stem_computer-science_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu Middle Stem Computer-Science Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Middle Stem Computer-Science Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_social-science_geography_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Social-Science Geography Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Social-Science Geography Egy evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_social-science_social-science_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Social-Science Social-Science Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Social-Science Social-Science Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_stem_natural-science_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Stem Natural-Science Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Stem Natural-Science Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_social-science_accounting_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu Univ Social-Science Accounting Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Univ Social-Science Accounting Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_social-science_political-science_egy\",\n",
      "      \"name\": \"Aradice Arabicmmlu Univ Social-Science Political-Science Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Univ Social-Science Political-Science Egy evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_stem_computer-science_lev\",\n",
      "      \"name\": \"Aradice Arabicmmlu Univ Stem Computer-Science Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Univ Stem Computer-Science Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_college_biology_light\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu College Biology Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu College Biology Light evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"agieval_logiqa_zh\",\n",
      "      \"name\": \"Agieval Logiqa Zh\",\n",
      "      \"description\": \"Agieval Logiqa Zh evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 651,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh\",\n",
      "      \"name\": \"Bbh\",\n",
      "      \"description\": \"Bbh evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_fewshot\",\n",
      "      \"name\": \"Bbh Cot Fewshot\",\n",
      "      \"description\": \"Bbh Cot Fewshot evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_fewshot_causal_judgement\",\n",
      "      \"name\": \"Bbh Cot Fewshot Causal Judgement\",\n",
      "      \"description\": \"Bbh Cot Fewshot Causal Judgement evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_fewshot_dyck_languages\",\n",
      "      \"name\": \"Bbh Cot Fewshot Dyck Languages\",\n",
      "      \"description\": \"Bbh Cot Fewshot Dyck Languages evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_fewshot_hyperbaton\",\n",
      "      \"name\": \"Bbh Cot Fewshot Hyperbaton\",\n",
      "      \"description\": \"Bbh Cot Fewshot Hyperbaton evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_fewshot_logical_deduction_three_objects\",\n",
      "      \"name\": \"Bbh Cot Fewshot Logical Deduction Three Objects\",\n",
      "      \"description\": \"Bbh Cot Fewshot Logical Deduction Three Objects evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_fewshot_navigate\",\n",
      "      \"name\": \"Bbh Cot Fewshot Navigate\",\n",
      "      \"description\": \"Bbh Cot Fewshot Navigate evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_fewshot_reasoning_about_colored_objects\",\n",
      "      \"name\": \"Bbh Cot Fewshot Reasoning About Colored Objects\",\n",
      "      \"description\": \"Bbh Cot Fewshot Reasoning About Colored Objects evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_fewshot_snarks\",\n",
      "      \"name\": \"Bbh Cot Fewshot Snarks\",\n",
      "      \"description\": \"Bbh Cot Fewshot Snarks evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_fewshot_tracking_shuffled_objects_five_objects\",\n",
      "      \"name\": \"Bbh Cot Fewshot Tracking Shuffled Objects Five Objects\",\n",
      "      \"description\": \"Bbh Cot Fewshot Tracking Shuffled Objects Five Objects evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_fewshot_web_of_lies\",\n",
      "      \"name\": \"Bbh Cot Fewshot Web Of Lies\",\n",
      "      \"description\": \"Bbh Cot Fewshot Web Of Lies evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_zeroshot\",\n",
      "      \"name\": \"Bbh Cot Zeroshot\",\n",
      "      \"description\": \"Bbh Cot Zeroshot evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_zeroshot_causal_judgement\",\n",
      "      \"name\": \"Bbh Cot Zeroshot Causal Judgement\",\n",
      "      \"description\": \"Bbh Cot Zeroshot Causal Judgement evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bbh_cot_zeroshot_dyck_languages\",\n",
      "      \"name\": \"Bbh Cot Zeroshot Dyck Languages\",\n",
      "      \"description\": \"Bbh Cot Zeroshot Dyck Languages evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_anatomy\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu Anatomy\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu Anatomy evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_clinical_knowledge\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu Clinical Knowledge\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu Clinical Knowledge evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_medical_genetics\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu Medical Genetics\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu Medical Genetics evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_professional_medicine\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu Professional Medicine\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu Professional Medicine evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"cmmlu_professional_medicine\",\n",
      "      \"name\": \"Cmmlu Professional Medicine\",\n",
      "      \"description\": \"Cmmlu Professional Medicine evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"cmmlu_traditional_chinese_medicine\",\n",
      "      \"name\": \"Cmmlu Traditional Chinese Medicine\",\n",
      "      \"description\": \"Cmmlu Traditional Chinese Medicine evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_am_anatomy\",\n",
      "      \"name\": \"Global Mmlu Full Am Anatomy\",\n",
      "      \"description\": \"Global Mmlu Full Am Anatomy evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_am_clinical_knowledge\",\n",
      "      \"name\": \"Global Mmlu Full Am Clinical Knowledge\",\n",
      "      \"description\": \"Global Mmlu Full Am Clinical Knowledge evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_am_medical_genetics\",\n",
      "      \"name\": \"Global Mmlu Full Am Medical Genetics\",\n",
      "      \"description\": \"Global Mmlu Full Am Medical Genetics evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_am_professional_medicine\",\n",
      "      \"name\": \"Global Mmlu Full Am Professional Medicine\",\n",
      "      \"description\": \"Global Mmlu Full Am Professional Medicine evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_ar_anatomy\",\n",
      "      \"name\": \"Global Mmlu Full Ar Anatomy\",\n",
      "      \"description\": \"Global Mmlu Full Ar Anatomy evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_ar_clinical_knowledge\",\n",
      "      \"name\": \"Global Mmlu Full Ar Clinical Knowledge\",\n",
      "      \"description\": \"Global Mmlu Full Ar Clinical Knowledge evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_ar_medical_genetics\",\n",
      "      \"name\": \"Global Mmlu Full Ar Medical Genetics\",\n",
      "      \"description\": \"Global Mmlu Full Ar Medical Genetics evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_ar_professional_medicine\",\n",
      "      \"name\": \"Global Mmlu Full Ar Professional Medicine\",\n",
      "      \"description\": \"Global Mmlu Full Ar Professional Medicine evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"global_mmlu_full_bn_anatomy\",\n",
      "      \"name\": \"Global Mmlu Full Bn Anatomy\",\n",
      "      \"description\": \"Global Mmlu Full Bn Anatomy evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lambada_openai\",\n",
      "      \"name\": \"Lambada Openai\",\n",
      "      \"description\": \"Lambada Openai evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"perplexity\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 5153,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lambada_openai_mt_en\",\n",
      "      \"name\": \"Lambada Openai Mt En\",\n",
      "      \"description\": \"Lambada Openai Mt En evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"perplexity\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 5153,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lambada_openai_mt_it\",\n",
      "      \"name\": \"Lambada Openai Mt It\",\n",
      "      \"description\": \"Lambada Openai Mt It evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"perplexity\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 5153,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lambada_openai_mt_stablelm_es\",\n",
      "      \"name\": \"Lambada Openai Mt Stablelm Es\",\n",
      "      \"description\": \"Lambada Openai Mt Stablelm Es evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"perplexity\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 5153,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lambada_openai_mt_stablelm_nl\",\n",
      "      \"name\": \"Lambada Openai Mt Stablelm Nl\",\n",
      "      \"description\": \"Lambada Openai Mt Stablelm Nl evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"perplexity\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 5153,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lambada_standard_cloze_yaml\",\n",
      "      \"name\": \"Lambada Standard Cloze Yaml\",\n",
      "      \"description\": \"Lambada Standard Cloze Yaml evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"perplexity\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 5153,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"paloma_wikitext_103\",\n",
      "      \"name\": \"Paloma Wikitext 103\",\n",
      "      \"description\": \"Paloma Wikitext 103 evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 4358,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"pile_arxiv\",\n",
      "      \"name\": \"Pile Arxiv\",\n",
      "      \"description\": \"Pile Arxiv evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 210000000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"pile_freelaw\",\n",
      "      \"name\": \"Pile Freelaw\",\n",
      "      \"description\": \"Pile Freelaw evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 210000000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"pile_hackernews\",\n",
      "      \"name\": \"Pile Hackernews\",\n",
      "      \"description\": \"Pile Hackernews evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 210000000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"pile_openwebtext2\",\n",
      "      \"name\": \"Pile Openwebtext2\",\n",
      "      \"description\": \"Pile Openwebtext2 evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 210000000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"pile_ubuntu-irc\",\n",
      "      \"name\": \"Pile Ubuntu-Irc\",\n",
      "      \"description\": \"Pile Ubuntu-Irc evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 210000000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"pile_youtubesubtitles\",\n",
      "      \"name\": \"Pile Youtubesubtitles\",\n",
      "      \"description\": \"Pile Youtubesubtitles evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 210000000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"wikitext\",\n",
      "      \"name\": \"Wikitext\",\n",
      "      \"description\": \"Wikitext evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 4358,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"careqa_open_perplexity\",\n",
      "      \"name\": \"Careqa Open Perplexity\",\n",
      "      \"description\": \"Careqa Open Perplexity evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"AraDiCE_truthfulqa_mc1_lev\",\n",
      "      \"name\": \"Aradice Truthfulqa Mc1 Lev\",\n",
      "      \"description\": \"Aradice Truthfulqa Mc1 Lev evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"metabench_truthfulqa_permute\",\n",
      "      \"name\": \"Metabench Truthfulqa Permute\",\n",
      "      \"description\": \"Metabench Truthfulqa Permute evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"nortruthfulqa_gen_nno_p0\",\n",
      "      \"name\": \"Nortruthfulqa Gen Nno P0\",\n",
      "      \"description\": \"Nortruthfulqa Gen Nno P0 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"nortruthfulqa_gen_nno_p3\",\n",
      "      \"name\": \"Nortruthfulqa Gen Nno P3\",\n",
      "      \"description\": \"Nortruthfulqa Gen Nno P3 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"nortruthfulqa_gen_nob_p1\",\n",
      "      \"name\": \"Nortruthfulqa Gen Nob P1\",\n",
      "      \"description\": \"Nortruthfulqa Gen Nob P1 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"nortruthfulqa_gen_nob_p4\",\n",
      "      \"name\": \"Nortruthfulqa Gen Nob P4\",\n",
      "      \"description\": \"Nortruthfulqa Gen Nob P4 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"nortruthfulqa_mc_nno_p2\",\n",
      "      \"name\": \"Nortruthfulqa Mc Nno P2\",\n",
      "      \"description\": \"Nortruthfulqa Mc Nno P2 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"nortruthfulqa_mc_nob_p0\",\n",
      "      \"name\": \"Nortruthfulqa Mc Nob P0\",\n",
      "      \"description\": \"Nortruthfulqa Mc Nob P0 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"nortruthfulqa_mc_nob_p3\",\n",
      "      \"name\": \"Nortruthfulqa Mc Nob P3\",\n",
      "      \"description\": \"Nortruthfulqa Mc Nob P3 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"tinyTruthfulQA\",\n",
      "      \"name\": \"Tinytruthfulqa\",\n",
      "      \"description\": \"Tinytruthfulqa evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"truthfulqa-multi_gen_ca\",\n",
      "      \"name\": \"Truthfulqa-Multi Gen Ca\",\n",
      "      \"description\": \"Truthfulqa-Multi Gen Ca evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"truthfulqa-multi_gen_eu\",\n",
      "      \"name\": \"Truthfulqa-Multi Gen Eu\",\n",
      "      \"description\": \"Truthfulqa-Multi Gen Eu evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"truthfulqa-multi_mc1_en\",\n",
      "      \"name\": \"Truthfulqa-Multi Mc1 En\",\n",
      "      \"description\": \"Truthfulqa-Multi Mc1 En evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"truthfulqa-multi_mc1_gl\",\n",
      "      \"name\": \"Truthfulqa-Multi Mc1 Gl\",\n",
      "      \"description\": \"Truthfulqa-Multi Mc1 Gl evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"truthfulqa-multi_mc2_es\",\n",
      "      \"name\": \"Truthfulqa-Multi Mc2 Es\",\n",
      "      \"description\": \"Truthfulqa-Multi Mc2 Es evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"bigbench_code_line_description_multiple_choice\",\n",
      "      \"name\": \"Bigbench Code Line Description Multiple Choice\",\n",
      "      \"description\": \"Bigbench Code Line Description Multiple Choice evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"ceval-valid_college_programming\",\n",
      "      \"name\": \"Ceval-Valid College Programming\",\n",
      "      \"description\": \"Ceval-Valid College Programming evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"code2text_javascript\",\n",
      "      \"name\": \"Code2Text Javascript\",\n",
      "      \"description\": \"Code2Text Javascript evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"code2text_ruby\",\n",
      "      \"name\": \"Code2Text Ruby\",\n",
      "      \"description\": \"Code2Text Ruby evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"humaneval\",\n",
      "      \"name\": \"Humaneval\",\n",
      "      \"description\": \"Humaneval evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"humaneval_instruct\",\n",
      "      \"name\": \"Humaneval Instruct\",\n",
      "      \"description\": \"Humaneval Instruct evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"mbpp\",\n",
      "      \"name\": \"MBPP\",\n",
      "      \"description\": \"MBPP (Most Basic Python Programming) evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "--------------------------------------------------\n",
      "Provider: LM Evaluation Harness\n",
      "Description: Comprehensive evaluation framework for language models with 167 benchmarks\n",
      "Number of benchmarks: 168\n"
     ]
    }
   ],
   "source": [
    "# Get details for the lm_evaluation_harness provider\n",
    "provider_id = \"lm_evaluation_harness\"\n",
    "response = api_request(\"GET\", f\"/providers/{provider_id}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    provider = response.json()\n",
    "    print(f\"Provider: {provider['provider_name']}\")\n",
    "    print(f\"Description: {provider['description']}\")\n",
    "    print(f\"Number of benchmarks: {len(provider['benchmarks'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Discovery\n",
    "\n",
    "### List All Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://localhost:8000/api/v1/benchmarks\n",
      "Status: 200\n",
      "Response:\n",
      "{\n",
      "  \"benchmarks\": [\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arc_easy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"ARC Easy\",\n",
      "      \"description\": \"ARC Easy evaluation benchmark - AI2 Reasoning Challenge (Easy)\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2376,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_boolq_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Boolq Lev\",\n",
      "      \"description\": \"Aradice Boolq Lev evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3270,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp\",\n",
      "      \"description\": \"Blimp evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_anaphor_gender_agreement\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Anaphor Gender Agreement\",\n",
      "      \"description\": \"Blimp Anaphor Gender Agreement evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_animate_subject_trans\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Animate Subject Trans\",\n",
      "      \"description\": \"Blimp Animate Subject Trans evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_coordinate_structure_constraint_complex_left_branch\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Coordinate Structure Constraint Complex Left Branch\",\n",
      "      \"description\": \"Blimp Coordinate Structure Constraint Complex Left Branch evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_determiner_noun_agreement_2\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Determiner Noun Agreement 2\",\n",
      "      \"description\": \"Blimp Determiner Noun Agreement 2 evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_determiner_noun_agreement_with_adj_2\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Determiner Noun Agreement With Adj 2\",\n",
      "      \"description\": \"Blimp Determiner Noun Agreement With Adj 2 evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_determiner_noun_agreement_with_adjective_1\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Determiner Noun Agreement With Adjective 1\",\n",
      "      \"description\": \"Blimp Determiner Noun Agreement With Adjective 1 evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_existential_there_object_raising\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Existential There Object Raising\",\n",
      "      \"description\": \"Blimp Existential There Object Raising evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_existential_there_subject_raising\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Existential There Subject Raising\",\n",
      "      \"description\": \"Blimp Existential There Subject Raising evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_intransitive\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Intransitive\",\n",
      "      \"description\": \"Blimp Intransitive evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_irregular_plural_subject_verb_agreement_1\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Irregular Plural Subject Verb Agreement 1\",\n",
      "      \"description\": \"Blimp Irregular Plural Subject Verb Agreement 1 evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_left_branch_island_simple_question\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Left Branch Island Simple Question\",\n",
      "      \"description\": \"Blimp Left Branch Island Simple Question evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_npi_present_2\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Npi Present 2\",\n",
      "      \"description\": \"Blimp Npi Present 2 evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_passive_1\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Passive 1\",\n",
      "      \"description\": \"Blimp Passive 1 evaluation benchmark\",\n",
      "      \"category\": \"general\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"general\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_humanities_history_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Humanities History Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Humanities History Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_humanities_philosophy_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Humanities Philosophy Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Humanities Philosophy Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_language_arabic-language_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Language Arabic-Language Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Language Arabic-Language Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_middle_humanities_islamic-studies_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Middle Humanities Islamic-Studies Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Middle Humanities Islamic-Studies Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_middle_language_arabic-language_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Middle Language Arabic-Language Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Middle Language Arabic-Language Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_na_humanities_islamic-studies_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Na Humanities Islamic-Studies Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Na Humanities Islamic-Studies Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_na_language_arabic-language-general_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Na Language Arabic-Language-General Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Na Language Arabic-Language-General Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_na_other_driving-test_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Na Other Driving-Test Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Na Other Driving-Test Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_na_other_general-knowledge_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Na Other General-Knowledge Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Na Other General-Knowledge Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_humanities_islamic-studies_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Humanities Islamic-Studies Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Humanities Islamic-Studies Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_language_arabic-language_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Language Arabic-Language Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Language Arabic-Language Lev evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_univ_other_management_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Univ Other Management Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Univ Other Management Egy evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_openbookqa_eng\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Openbookqa Eng\",\n",
      "      \"description\": \"Aradice Openbookqa Eng evaluation benchmark\",\n",
      "      \"category\": \"knowledge\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 500,\n",
      "      \"tags\": [\n",
      "        \"knowledge\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_boolq\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Boolq\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Boolq evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3270,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_boolq_light\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Boolq Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Boolq Light evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3270,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_mt_boolq_light\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Mt Boolq Light\",\n",
      "      \"description\": \"Arabic Mt Boolq Light evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3270,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::leaderboard_bbh_salient_translation_error_detection\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Leaderboard Bbh Salient Translation Error Detection\",\n",
      "      \"description\": \"Leaderboard Bbh Salient Translation Error Detection evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"bleu\",\n",
      "        \"chrf\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::aclue_ancient_chinese_culture\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aclue Ancient Chinese Culture\",\n",
      "      \"description\": \"Aclue Ancient Chinese Culture evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::african_flores\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"African Flores\",\n",
      "      \"description\": \"African Flores evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"bleu\",\n",
      "        \"chrf\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli-irokobench\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Afrixnli-Irokobench\",\n",
      "      \"description\": \"Afrixnli-Irokobench evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_amh_prompt_2\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Afrixnli Amh Prompt 2\",\n",
      "      \"description\": \"Afrixnli Amh Prompt 2 evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_amh_prompt_5\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Afrixnli Amh Prompt 5\",\n",
      "      \"description\": \"Afrixnli Amh Prompt 5 evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_en_direct_ewe\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Afrixnli En Direct Ewe\",\n",
      "      \"description\": \"Afrixnli En Direct Ewe evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_en_direct_ibo\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Afrixnli En Direct Ibo\",\n",
      "      \"description\": \"Afrixnli En Direct Ibo evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_en_direct_lug\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Afrixnli En Direct Lug\",\n",
      "      \"description\": \"Afrixnli En Direct Lug evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_en_direct_sot\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Afrixnli En Direct Sot\",\n",
      "      \"description\": \"Afrixnli En Direct Sot evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_en_direct_wol\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Afrixnli En Direct Wol\",\n",
      "      \"description\": \"Afrixnli En Direct Wol evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::afrixnli_en_direct_zul\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Afrixnli En Direct Zul\",\n",
      "      \"description\": \"Afrixnli En Direct Zul evaluation benchmark\",\n",
      "      \"category\": \"multilingual\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 2000,\n",
      "      \"tags\": [\n",
      "        \"multilingual\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_stem_math_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Stem Math Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Stem Math Egy evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_college_mathematics_light\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::cmmlu_college_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Cmmlu College Mathematics\",\n",
      "      \"description\": \"Cmmlu College Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::cmmlu_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Cmmlu High School Mathematics\",\n",
      "      \"description\": \"Cmmlu High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_am_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Am High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Am High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_ar_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Ar High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Ar High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_bn_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Bn High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Bn High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_cs_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Cs High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Cs High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_de_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full De High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full De High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_el_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full El High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full El High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_en_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full En High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full En High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_es_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Es High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Es High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_fa_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Fa High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Fa High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_fil_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Fil High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Fil High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_piqa_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Piqa Lev\",\n",
      "      \"description\": \"Aradice Piqa Lev evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1838,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_winogrande_eng\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Winogrande Eng\",\n",
      "      \"description\": \"Aradice Winogrande Eng evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1267,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_copa\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Copa\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Copa evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 500,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_copa_light\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Copa Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Copa Light evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 500,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_hellaswag\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Hellaswag\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Hellaswag evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10042,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_hellaswag_light\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Hellaswag Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Hellaswag Light evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10042,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_piqa\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Piqa\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Piqa evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1838,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_piqa_light\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Piqa Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Piqa Light evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1838,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_mt_hellaswag\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Mt Hellaswag\",\n",
      "      \"description\": \"Arabic Mt Hellaswag evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10042,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_mt_piqa\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Mt Piqa\",\n",
      "      \"description\": \"Arabic Mt Piqa evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1838,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::copa_ar\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Copa Ar\",\n",
      "      \"description\": \"Copa Ar evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 500,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::copal_id_colloquial\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Copal Id Colloquial\",\n",
      "      \"description\": \"Copal Id Colloquial evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 500,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::darijahellaswag\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Darijahellaswag\",\n",
      "      \"description\": \"Darijahellaswag evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10042,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::egyhellaswag\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Egyhellaswag\",\n",
      "      \"description\": \"Egyhellaswag evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10042,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::hellaswag_ar\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Hellaswag Ar\",\n",
      "      \"description\": \"Hellaswag Ar evaluation benchmark\",\n",
      "      \"category\": \"reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10042,\n",
      "      \"tags\": [\n",
      "        \"reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_race\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Race\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Race evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 674,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mt_race_light\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mt Race Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mt Race Light evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 674,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_mt_race_light\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Mt Race Light\",\n",
      "      \"description\": \"Arabic Mt Race Light evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 674,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::blimp_drop_argument\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Blimp Drop Argument\",\n",
      "      \"description\": \"Blimp Drop Argument evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 9536,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bigbench_gre_reading_comprehension_multiple_choice\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bigbench Gre Reading Comprehension Multiple Choice\",\n",
      "      \"description\": \"Bigbench Gre Reading Comprehension Multiple Choice evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3000,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::eus_reading\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Eus Reading\",\n",
      "      \"description\": \"Eus Reading evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3000,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::longbench_qasper\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Longbench Qasper\",\n",
      "      \"description\": \"Longbench Qasper evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3000,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::qasper_freeform\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Qasper Freeform\",\n",
      "      \"description\": \"Qasper Freeform evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3000,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::ruler_qa_squad\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Ruler Qa Squad\",\n",
      "      \"description\": \"Ruler Qa Squad evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3000,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::scrolls_qasper\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Scrolls Qasper\",\n",
      "      \"description\": \"Scrolls Qasper evaluation benchmark\",\n",
      "      \"category\": \"reading_comprehension\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 3000,\n",
      "      \"tags\": [\n",
      "        \"reading_comprehension\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_social-science_economics_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Social-Science Economics Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Social-Science Economics Egy evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_social-science_geography_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Social-Science Geography Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Social-Science Geography Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_stem_computer-science_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Stem Computer-Science Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Stem Computer-Science Egy evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_high_stem_physics_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu High Stem Physics Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu High Stem Physics Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_middle_social-science_civics_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Middle Social-Science Civics Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Middle Social-Science Civics Egy evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_middle_social-science_economics_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Middle Social-Science Economics Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Middle Social-Science Economics Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_middle_social-science_social-science_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Middle Social-Science Social-Science Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Middle Social-Science Social-Science Egy evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_middle_stem_computer-science_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Middle Stem Computer-Science Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Middle Stem Computer-Science Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_social-science_geography_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Social-Science Geography Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Social-Science Geography Egy evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_social-science_social-science_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Social-Science Social-Science Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Social-Science Social-Science Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_stem_natural-science_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Stem Natural-Science Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Stem Natural-Science Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_univ_social-science_accounting_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Univ Social-Science Accounting Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Univ Social-Science Accounting Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_univ_social-science_political-science_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Univ Social-Science Political-Science Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Univ Social-Science Political-Science Egy evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_univ_stem_computer-science_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Univ Stem Computer-Science Lev\",\n",
      "      \"description\": \"Aradice Arabicmmlu Univ Stem Computer-Science Lev evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_college_biology_light\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu College Biology Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu College Biology Light evaluation benchmark\",\n",
      "      \"category\": \"science\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"science\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::agieval_logiqa_zh\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Agieval Logiqa Zh\",\n",
      "      \"description\": \"Agieval Logiqa Zh evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 651,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh\",\n",
      "      \"description\": \"Bbh evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Fewshot\",\n",
      "      \"description\": \"Bbh Cot Fewshot evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_causal_judgement\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Fewshot Causal Judgement\",\n",
      "      \"description\": \"Bbh Cot Fewshot Causal Judgement evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_dyck_languages\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Fewshot Dyck Languages\",\n",
      "      \"description\": \"Bbh Cot Fewshot Dyck Languages evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_hyperbaton\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Fewshot Hyperbaton\",\n",
      "      \"description\": \"Bbh Cot Fewshot Hyperbaton evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_logical_deduction_three_objects\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Fewshot Logical Deduction Three Objects\",\n",
      "      \"description\": \"Bbh Cot Fewshot Logical Deduction Three Objects evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_navigate\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Fewshot Navigate\",\n",
      "      \"description\": \"Bbh Cot Fewshot Navigate evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_reasoning_about_colored_objects\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Fewshot Reasoning About Colored Objects\",\n",
      "      \"description\": \"Bbh Cot Fewshot Reasoning About Colored Objects evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_snarks\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Fewshot Snarks\",\n",
      "      \"description\": \"Bbh Cot Fewshot Snarks evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_tracking_shuffled_objects_five_objects\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Fewshot Tracking Shuffled Objects Five Objects\",\n",
      "      \"description\": \"Bbh Cot Fewshot Tracking Shuffled Objects Five Objects evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_fewshot_web_of_lies\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Fewshot Web Of Lies\",\n",
      "      \"description\": \"Bbh Cot Fewshot Web Of Lies evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 5,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_zeroshot\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Zeroshot\",\n",
      "      \"description\": \"Bbh Cot Zeroshot evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_zeroshot_causal_judgement\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Zeroshot Causal Judgement\",\n",
      "      \"description\": \"Bbh Cot Zeroshot Causal Judgement evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bbh_cot_zeroshot_dyck_languages\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bbh Cot Zeroshot Dyck Languages\",\n",
      "      \"description\": \"Bbh Cot Zeroshot Dyck Languages evaluation benchmark\",\n",
      "      \"category\": \"logic_reasoning\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"logic_reasoning\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_anatomy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu Anatomy\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu Anatomy evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_clinical_knowledge\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu Clinical Knowledge\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu Clinical Knowledge evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_medical_genetics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu Medical Genetics\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu Medical Genetics evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_professional_medicine\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu Professional Medicine\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu Professional Medicine evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::cmmlu_professional_medicine\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Cmmlu Professional Medicine\",\n",
      "      \"description\": \"Cmmlu Professional Medicine evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::cmmlu_traditional_chinese_medicine\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Cmmlu Traditional Chinese Medicine\",\n",
      "      \"description\": \"Cmmlu Traditional Chinese Medicine evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_am_anatomy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Am Anatomy\",\n",
      "      \"description\": \"Global Mmlu Full Am Anatomy evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_am_clinical_knowledge\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Am Clinical Knowledge\",\n",
      "      \"description\": \"Global Mmlu Full Am Clinical Knowledge evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_am_medical_genetics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Am Medical Genetics\",\n",
      "      \"description\": \"Global Mmlu Full Am Medical Genetics evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_am_professional_medicine\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Am Professional Medicine\",\n",
      "      \"description\": \"Global Mmlu Full Am Professional Medicine evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_ar_anatomy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Ar Anatomy\",\n",
      "      \"description\": \"Global Mmlu Full Ar Anatomy evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_ar_clinical_knowledge\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Ar Clinical Knowledge\",\n",
      "      \"description\": \"Global Mmlu Full Ar Clinical Knowledge evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_ar_medical_genetics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Ar Medical Genetics\",\n",
      "      \"description\": \"Global Mmlu Full Ar Medical Genetics evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_ar_professional_medicine\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Ar Professional Medicine\",\n",
      "      \"description\": \"Global Mmlu Full Ar Professional Medicine evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_bn_anatomy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Bn Anatomy\",\n",
      "      \"description\": \"Global Mmlu Full Bn Anatomy evaluation benchmark\",\n",
      "      \"category\": \"medical\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"medical\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::lambada_openai\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Lambada Openai\",\n",
      "      \"description\": \"Lambada Openai evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"perplexity\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 5153,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::lambada_openai_mt_en\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Lambada Openai Mt En\",\n",
      "      \"description\": \"Lambada Openai Mt En evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"perplexity\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 5153,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::lambada_openai_mt_it\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Lambada Openai Mt It\",\n",
      "      \"description\": \"Lambada Openai Mt It evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"perplexity\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 5153,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::lambada_openai_mt_stablelm_es\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Lambada Openai Mt Stablelm Es\",\n",
      "      \"description\": \"Lambada Openai Mt Stablelm Es evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"perplexity\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 5153,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::lambada_openai_mt_stablelm_nl\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Lambada Openai Mt Stablelm Nl\",\n",
      "      \"description\": \"Lambada Openai Mt Stablelm Nl evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"perplexity\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 5153,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::lambada_standard_cloze_yaml\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Lambada Standard Cloze Yaml\",\n",
      "      \"description\": \"Lambada Standard Cloze Yaml evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"perplexity\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 5153,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::paloma_wikitext_103\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Paloma Wikitext 103\",\n",
      "      \"description\": \"Paloma Wikitext 103 evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 4358,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::pile_arxiv\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Pile Arxiv\",\n",
      "      \"description\": \"Pile Arxiv evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 210000000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::pile_freelaw\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Pile Freelaw\",\n",
      "      \"description\": \"Pile Freelaw evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 210000000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::pile_hackernews\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Pile Hackernews\",\n",
      "      \"description\": \"Pile Hackernews evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 210000000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::pile_openwebtext2\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Pile Openwebtext2\",\n",
      "      \"description\": \"Pile Openwebtext2 evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 210000000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::pile_ubuntu-irc\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Pile Ubuntu-Irc\",\n",
      "      \"description\": \"Pile Ubuntu-Irc evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 210000000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::pile_youtubesubtitles\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Pile Youtubesubtitles\",\n",
      "      \"description\": \"Pile Youtubesubtitles evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 210000000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::wikitext\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Wikitext\",\n",
      "      \"description\": \"Wikitext evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 4358,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::careqa_open_perplexity\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Careqa Open Perplexity\",\n",
      "      \"description\": \"Careqa Open Perplexity evaluation benchmark\",\n",
      "      \"category\": \"language_modeling\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 10000,\n",
      "      \"tags\": [\n",
      "        \"language_modeling\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_truthfulqa_mc1_lev\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Truthfulqa Mc1 Lev\",\n",
      "      \"description\": \"Aradice Truthfulqa Mc1 Lev evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::metabench_truthfulqa_permute\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Metabench Truthfulqa Permute\",\n",
      "      \"description\": \"Metabench Truthfulqa Permute evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_gen_nno_p0\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Nortruthfulqa Gen Nno P0\",\n",
      "      \"description\": \"Nortruthfulqa Gen Nno P0 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_gen_nno_p3\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Nortruthfulqa Gen Nno P3\",\n",
      "      \"description\": \"Nortruthfulqa Gen Nno P3 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_gen_nob_p1\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Nortruthfulqa Gen Nob P1\",\n",
      "      \"description\": \"Nortruthfulqa Gen Nob P1 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_gen_nob_p4\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Nortruthfulqa Gen Nob P4\",\n",
      "      \"description\": \"Nortruthfulqa Gen Nob P4 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_mc_nno_p2\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Nortruthfulqa Mc Nno P2\",\n",
      "      \"description\": \"Nortruthfulqa Mc Nno P2 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_mc_nob_p0\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Nortruthfulqa Mc Nob P0\",\n",
      "      \"description\": \"Nortruthfulqa Mc Nob P0 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::nortruthfulqa_mc_nob_p3\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Nortruthfulqa Mc Nob P3\",\n",
      "      \"description\": \"Nortruthfulqa Mc Nob P3 evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::tinyTruthfulQA\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Tinytruthfulqa\",\n",
      "      \"description\": \"Tinytruthfulqa evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::truthfulqa-multi_gen_ca\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Truthfulqa-Multi Gen Ca\",\n",
      "      \"description\": \"Truthfulqa-Multi Gen Ca evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::truthfulqa-multi_gen_eu\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Truthfulqa-Multi Gen Eu\",\n",
      "      \"description\": \"Truthfulqa-Multi Gen Eu evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"mc1\",\n",
      "        \"mc2\",\n",
      "        \"bleu\",\n",
      "        \"rouge\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::truthfulqa-multi_mc1_en\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Truthfulqa-Multi Mc1 En\",\n",
      "      \"description\": \"Truthfulqa-Multi Mc1 En evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::truthfulqa-multi_mc1_gl\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Truthfulqa-Multi Mc1 Gl\",\n",
      "      \"description\": \"Truthfulqa-Multi Mc1 Gl evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::truthfulqa-multi_mc2_es\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Truthfulqa-Multi Mc2 Es\",\n",
      "      \"description\": \"Truthfulqa-Multi Mc2 Es evaluation benchmark\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 817,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::bigbench_code_line_description_multiple_choice\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Bigbench Code Line Description Multiple Choice\",\n",
      "      \"description\": \"Bigbench Code Line Description Multiple Choice evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\",\n",
      "        \"acc_norm\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::ceval-valid_college_programming\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Ceval-Valid College Programming\",\n",
      "      \"description\": \"Ceval-Valid College Programming evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::code2text_javascript\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Code2Text Javascript\",\n",
      "      \"description\": \"Code2Text Javascript evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::code2text_ruby\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Code2Text Ruby\",\n",
      "      \"description\": \"Code2Text Ruby evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::humaneval\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Humaneval\",\n",
      "      \"description\": \"Humaneval evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::humaneval_instruct\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Humaneval Instruct\",\n",
      "      \"description\": \"Humaneval Instruct evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::mbpp\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"MBPP\",\n",
      "      \"description\": \"MBPP (Most Basic Python Programming) evaluation benchmark\",\n",
      "      \"category\": \"code\",\n",
      "      \"metrics\": [\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"code\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"ragas::faithfulness\",\n",
      "      \"provider_id\": \"ragas\",\n",
      "      \"name\": \"Faithfulness\",\n",
      "      \"description\": \"Measures factual consistency of generated answer against given context\",\n",
      "      \"category\": \"rag_quality\",\n",
      "      \"metrics\": [\n",
      "        \"faithfulness_score\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": null,\n",
      "      \"tags\": [\n",
      "        \"rag\",\n",
      "        \"faithfulness\",\n",
      "        \"factuality\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"ragas::answer_relevancy\",\n",
      "      \"provider_id\": \"ragas\",\n",
      "      \"name\": \"Answer Relevancy\",\n",
      "      \"description\": \"Measures how relevant generated answer is to the question\",\n",
      "      \"category\": \"rag_quality\",\n",
      "      \"metrics\": [\n",
      "        \"answer_relevancy_score\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": null,\n",
      "      \"tags\": [\n",
      "        \"rag\",\n",
      "        \"relevancy\",\n",
      "        \"quality\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"ragas::context_precision\",\n",
      "      \"provider_id\": \"ragas\",\n",
      "      \"name\": \"Context Precision\",\n",
      "      \"description\": \"Measures precision of retrieved context\",\n",
      "      \"category\": \"rag_retrieval\",\n",
      "      \"metrics\": [\n",
      "        \"context_precision_score\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": null,\n",
      "      \"tags\": [\n",
      "        \"rag\",\n",
      "        \"retrieval\",\n",
      "        \"precision\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"ragas::context_recall\",\n",
      "      \"provider_id\": \"ragas\",\n",
      "      \"name\": \"Context Recall\",\n",
      "      \"description\": \"Measures recall of retrieved context\",\n",
      "      \"category\": \"rag_retrieval\",\n",
      "      \"metrics\": [\n",
      "        \"context_recall_score\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": null,\n",
      "      \"tags\": [\n",
      "        \"rag\",\n",
      "        \"retrieval\",\n",
      "        \"recall\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"garak::toxicity\",\n",
      "      \"provider_id\": \"garak\",\n",
      "      \"name\": \"Toxicity Detection\",\n",
      "      \"description\": \"Tests model's tendency to generate toxic content\",\n",
      "      \"category\": \"safety\",\n",
      "      \"metrics\": [\n",
      "        \"toxicity_rate\",\n",
      "        \"severity_score\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 500,\n",
      "      \"tags\": [\n",
      "        \"safety\",\n",
      "        \"toxicity\",\n",
      "        \"red_team\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"garak::bias_detection\",\n",
      "      \"provider_id\": \"garak\",\n",
      "      \"name\": \"Bias Detection\",\n",
      "      \"description\": \"Evaluates model for various forms of bias\",\n",
      "      \"category\": \"fairness\",\n",
      "      \"metrics\": [\n",
      "        \"bias_score\",\n",
      "        \"demographic_parity\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 1000,\n",
      "      \"tags\": [\n",
      "        \"fairness\",\n",
      "        \"bias\",\n",
      "        \"demographic\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"garak::pii_leakage\",\n",
      "      \"provider_id\": \"garak\",\n",
      "      \"name\": \"PII Leakage\",\n",
      "      \"description\": \"Tests for personally identifiable information leakage\",\n",
      "      \"category\": \"privacy\",\n",
      "      \"metrics\": [\n",
      "        \"pii_leak_rate\",\n",
      "        \"sensitivity_score\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 300,\n",
      "      \"tags\": [\n",
      "        \"privacy\",\n",
      "        \"pii\",\n",
      "        \"security\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"garak::prompt_injection\",\n",
      "      \"provider_id\": \"garak\",\n",
      "      \"name\": \"Prompt Injection\",\n",
      "      \"description\": \"Tests resilience against prompt injection attacks\",\n",
      "      \"category\": \"security\",\n",
      "      \"metrics\": [\n",
      "        \"injection_success_rate\",\n",
      "        \"defense_effectiveness\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 200,\n",
      "      \"tags\": [\n",
      "        \"security\",\n",
      "        \"injection\",\n",
      "        \"adversarial\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"total_count\": 176,\n",
      "  \"providers_included\": [\n",
      "    \"lm_evaluation_harness\",\n",
      "    \"ragas\",\n",
      "    \"garak\"\n",
      "  ]\n",
      "}\n",
      "--------------------------------------------------\n",
      "Total benchmarks available: 176\n",
      "  - ARC Easy (lm_evaluation_harness::arc_easy)\n",
      "    Category: reasoning\n",
      "    Provider: lm_evaluation_harness\n",
      "  - Aradice Boolq Lev (lm_evaluation_harness::AraDiCE_boolq_lev)\n",
      "    Category: general\n",
      "    Provider: lm_evaluation_harness\n",
      "  - Blimp (lm_evaluation_harness::blimp)\n",
      "    Category: general\n",
      "    Provider: lm_evaluation_harness\n",
      "  - Blimp Anaphor Gender Agreement (lm_evaluation_harness::blimp_anaphor_gender_agreement)\n",
      "    Category: general\n",
      "    Provider: lm_evaluation_harness\n",
      "  - Blimp Animate Subject Trans (lm_evaluation_harness::blimp_animate_subject_trans)\n",
      "    Category: general\n",
      "    Provider: lm_evaluation_harness\n"
     ]
    }
   ],
   "source": [
    "response = api_request(\"GET\", \"/benchmarks\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    benchmarks_data = response.json()\n",
    "    print(f\"Total benchmarks available: {benchmarks_data['total_count']}\")\n",
    "\n",
    "    # Show first 5 benchmarks\n",
    "    for benchmark in benchmarks_data['benchmarks'][:5]:\n",
    "        print(f\"  - {benchmark['name']} ({benchmark['benchmark_id']})\")\n",
    "        print(f\"    Category: {benchmark['category']}\")\n",
    "        print(f\"    Provider: {benchmark['provider_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Benchmarks by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://localhost:8000/api/v1/benchmarks\n",
      "Status: 200\n",
      "Response:\n",
      "{\n",
      "  \"benchmarks\": [\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::AraDiCE_ArabicMMLU_primary_stem_math_egy\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Aradice Arabicmmlu Primary Stem Math Egy\",\n",
      "      \"description\": \"Aradice Arabicmmlu Primary Stem Math Egy evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_college_mathematics_light\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::arabic_leaderboard_arabic_mmlu_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics\",\n",
      "      \"description\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::cmmlu_college_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Cmmlu College Mathematics\",\n",
      "      \"description\": \"Cmmlu College Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::cmmlu_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Cmmlu High School Mathematics\",\n",
      "      \"description\": \"Cmmlu High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_am_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Am High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Am High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_ar_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Ar High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Ar High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_bn_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Bn High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Bn High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_cs_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Cs High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Cs High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_de_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full De High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full De High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_el_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full El High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full El High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_en_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full En High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full En High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_es_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Es High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Es High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_fa_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Fa High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Fa High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"benchmark_id\": \"lm_evaluation_harness::global_mmlu_full_fil_high_school_mathematics\",\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"name\": \"Global Mmlu Full Fil High School Mathematics\",\n",
      "      \"description\": \"Global Mmlu Full Fil High School Mathematics evaluation benchmark\",\n",
      "      \"category\": \"math\",\n",
      "      \"metrics\": [\n",
      "        \"exact_match\",\n",
      "        \"accuracy\"\n",
      "      ],\n",
      "      \"num_few_shot\": 0,\n",
      "      \"dataset_size\": 14042,\n",
      "      \"tags\": [\n",
      "        \"math\",\n",
      "        \"lm_eval\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"total_count\": 15,\n",
      "  \"providers_included\": [\n",
      "    \"lm_evaluation_harness\"\n",
      "  ]\n",
      "}\n",
      "--------------------------------------------------\n",
      "Math benchmarks: 15\n",
      "  - Aradice Arabicmmlu Primary Stem Math Egy: Aradice Arabicmmlu Primary Stem Math Egy evaluation benchmark\n",
      "  - Arabic Leaderboard Arabic Mmlu College Mathematics Light: Arabic Leaderboard Arabic Mmlu College Mathematics Light evaluation benchmark\n",
      "  - Arabic Leaderboard Arabic Mmlu High School Mathematics: Arabic Leaderboard Arabic Mmlu High School Mathematics evaluation benchmark\n",
      "  - Cmmlu College Mathematics: Cmmlu College Mathematics evaluation benchmark\n",
      "  - Cmmlu High School Mathematics: Cmmlu High School Mathematics evaluation benchmark\n",
      "  - Global Mmlu Full Am High School Mathematics: Global Mmlu Full Am High School Mathematics evaluation benchmark\n",
      "  - Global Mmlu Full Ar High School Mathematics: Global Mmlu Full Ar High School Mathematics evaluation benchmark\n",
      "  - Global Mmlu Full Bn High School Mathematics: Global Mmlu Full Bn High School Mathematics evaluation benchmark\n",
      "  - Global Mmlu Full Cs High School Mathematics: Global Mmlu Full Cs High School Mathematics evaluation benchmark\n",
      "  - Global Mmlu Full De High School Mathematics: Global Mmlu Full De High School Mathematics evaluation benchmark\n",
      "  - Global Mmlu Full El High School Mathematics: Global Mmlu Full El High School Mathematics evaluation benchmark\n",
      "  - Global Mmlu Full En High School Mathematics: Global Mmlu Full En High School Mathematics evaluation benchmark\n",
      "  - Global Mmlu Full Es High School Mathematics: Global Mmlu Full Es High School Mathematics evaluation benchmark\n",
      "  - Global Mmlu Full Fa High School Mathematics: Global Mmlu Full Fa High School Mathematics evaluation benchmark\n",
      "  - Global Mmlu Full Fil High School Mathematics: Global Mmlu Full Fil High School Mathematics evaluation benchmark\n"
     ]
    }
   ],
   "source": [
    "response = api_request(\"GET\", \"/benchmarks\", params={\"category\": \"math\"})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    math_benchmarks = response.json()\n",
    "    print(f\"Math benchmarks: {math_benchmarks['total_count']}\")\n",
    "    for benchmark in math_benchmarks['benchmarks']:\n",
    "        print(f\"  - {benchmark['name']}: {benchmark['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Provider-Specific Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://localhost:8000/api/v1/providers/lm_evaluation_harness/benchmarks\n",
      "Status: 200\n",
      "Response:\n",
      "[\n",
      "  {\n",
      "    \"benchmark_id\": \"arc_easy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"ARC Easy\",\n",
      "    \"description\": \"ARC Easy evaluation benchmark - AI2 Reasoning Challenge (Easy)\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2376,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_boolq_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Boolq Lev\",\n",
      "    \"description\": \"Aradice Boolq Lev evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 3270,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp\",\n",
      "    \"description\": \"Blimp evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_anaphor_gender_agreement\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Anaphor Gender Agreement\",\n",
      "    \"description\": \"Blimp Anaphor Gender Agreement evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_animate_subject_trans\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Animate Subject Trans\",\n",
      "    \"description\": \"Blimp Animate Subject Trans evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_coordinate_structure_constraint_complex_left_branch\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Coordinate Structure Constraint Complex Left Branch\",\n",
      "    \"description\": \"Blimp Coordinate Structure Constraint Complex Left Branch evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_determiner_noun_agreement_2\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Determiner Noun Agreement 2\",\n",
      "    \"description\": \"Blimp Determiner Noun Agreement 2 evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_determiner_noun_agreement_with_adj_2\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Determiner Noun Agreement With Adj 2\",\n",
      "    \"description\": \"Blimp Determiner Noun Agreement With Adj 2 evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_determiner_noun_agreement_with_adjective_1\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Determiner Noun Agreement With Adjective 1\",\n",
      "    \"description\": \"Blimp Determiner Noun Agreement With Adjective 1 evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_existential_there_object_raising\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Existential There Object Raising\",\n",
      "    \"description\": \"Blimp Existential There Object Raising evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_existential_there_subject_raising\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Existential There Subject Raising\",\n",
      "    \"description\": \"Blimp Existential There Subject Raising evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_intransitive\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Intransitive\",\n",
      "    \"description\": \"Blimp Intransitive evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_irregular_plural_subject_verb_agreement_1\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Irregular Plural Subject Verb Agreement 1\",\n",
      "    \"description\": \"Blimp Irregular Plural Subject Verb Agreement 1 evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_left_branch_island_simple_question\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Left Branch Island Simple Question\",\n",
      "    \"description\": \"Blimp Left Branch Island Simple Question evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_npi_present_2\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Npi Present 2\",\n",
      "    \"description\": \"Blimp Npi Present 2 evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_passive_1\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Passive 1\",\n",
      "    \"description\": \"Blimp Passive 1 evaluation benchmark\",\n",
      "    \"category\": \"general\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"general\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu Egy evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_humanities_history_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu High Humanities History Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu High Humanities History Lev evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_humanities_philosophy_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu High Humanities Philosophy Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu High Humanities Philosophy Egy evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_language_arabic-language_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu High Language Arabic-Language Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu High Language Arabic-Language Lev evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu Lev evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_humanities_islamic-studies_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Middle Humanities Islamic-Studies Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu Middle Humanities Islamic-Studies Egy evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_language_arabic-language_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Middle Language Arabic-Language Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu Middle Language Arabic-Language Lev evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_humanities_islamic-studies_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Na Humanities Islamic-Studies Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu Na Humanities Islamic-Studies Egy evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_language_arabic-language-general_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Na Language Arabic-Language-General Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu Na Language Arabic-Language-General Lev evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_other_driving-test_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Na Other Driving-Test Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu Na Other Driving-Test Egy evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_na_other_general-knowledge_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Na Other General-Knowledge Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu Na Other General-Knowledge Lev evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_humanities_islamic-studies_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Primary Humanities Islamic-Studies Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu Primary Humanities Islamic-Studies Egy evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_language_arabic-language_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Primary Language Arabic-Language Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu Primary Language Arabic-Language Lev evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_other_management_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Univ Other Management Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu Univ Other Management Egy evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_openbookqa_eng\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Openbookqa Eng\",\n",
      "    \"description\": \"Aradice Openbookqa Eng evaluation benchmark\",\n",
      "    \"category\": \"knowledge\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 500,\n",
      "    \"tags\": [\n",
      "      \"knowledge\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_boolq\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mt Boolq\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mt Boolq evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 3270,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_boolq_light\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mt Boolq Light\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mt Boolq Light evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 3270,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_mt_boolq_light\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Mt Boolq Light\",\n",
      "    \"description\": \"Arabic Mt Boolq Light evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 3270,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"leaderboard_bbh_salient_translation_error_detection\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Leaderboard Bbh Salient Translation Error Detection\",\n",
      "    \"description\": \"Leaderboard Bbh Salient Translation Error Detection evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"bleu\",\n",
      "      \"chrf\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2000,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"aclue_ancient_chinese_culture\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aclue Ancient Chinese Culture\",\n",
      "    \"description\": \"Aclue Ancient Chinese Culture evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2000,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"african_flores\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"African Flores\",\n",
      "    \"description\": \"African Flores evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"bleu\",\n",
      "      \"chrf\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2000,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"afrixnli-irokobench\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Afrixnli-Irokobench\",\n",
      "    \"description\": \"Afrixnli-Irokobench evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2000,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"afrixnli_amh_prompt_2\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Afrixnli Amh Prompt 2\",\n",
      "    \"description\": \"Afrixnli Amh Prompt 2 evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2000,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"afrixnli_amh_prompt_5\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Afrixnli Amh Prompt 5\",\n",
      "    \"description\": \"Afrixnli Amh Prompt 5 evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2000,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"afrixnli_en_direct_ewe\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Afrixnli En Direct Ewe\",\n",
      "    \"description\": \"Afrixnli En Direct Ewe evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2000,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"afrixnli_en_direct_ibo\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Afrixnli En Direct Ibo\",\n",
      "    \"description\": \"Afrixnli En Direct Ibo evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2000,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"afrixnli_en_direct_lug\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Afrixnli En Direct Lug\",\n",
      "    \"description\": \"Afrixnli En Direct Lug evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2000,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"afrixnli_en_direct_sot\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Afrixnli En Direct Sot\",\n",
      "    \"description\": \"Afrixnli En Direct Sot evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2000,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"afrixnli_en_direct_wol\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Afrixnli En Direct Wol\",\n",
      "    \"description\": \"Afrixnli En Direct Wol evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2000,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"afrixnli_en_direct_zul\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Afrixnli En Direct Zul\",\n",
      "    \"description\": \"Afrixnli En Direct Zul evaluation benchmark\",\n",
      "    \"category\": \"multilingual\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 2000,\n",
      "    \"tags\": [\n",
      "      \"multilingual\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_stem_math_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Primary Stem Math Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu Primary Stem Math Egy evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_college_mathematics_light\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mmlu College Mathematics Light evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_high_school_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mmlu High School Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"cmmlu_college_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Cmmlu College Mathematics\",\n",
      "    \"description\": \"Cmmlu College Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"cmmlu_high_school_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Cmmlu High School Mathematics\",\n",
      "    \"description\": \"Cmmlu High School Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_am_high_school_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Am High School Mathematics\",\n",
      "    \"description\": \"Global Mmlu Full Am High School Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_ar_high_school_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Ar High School Mathematics\",\n",
      "    \"description\": \"Global Mmlu Full Ar High School Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_bn_high_school_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Bn High School Mathematics\",\n",
      "    \"description\": \"Global Mmlu Full Bn High School Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_cs_high_school_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Cs High School Mathematics\",\n",
      "    \"description\": \"Global Mmlu Full Cs High School Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_de_high_school_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full De High School Mathematics\",\n",
      "    \"description\": \"Global Mmlu Full De High School Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_el_high_school_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full El High School Mathematics\",\n",
      "    \"description\": \"Global Mmlu Full El High School Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_en_high_school_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full En High School Mathematics\",\n",
      "    \"description\": \"Global Mmlu Full En High School Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_es_high_school_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Es High School Mathematics\",\n",
      "    \"description\": \"Global Mmlu Full Es High School Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_fa_high_school_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Fa High School Mathematics\",\n",
      "    \"description\": \"Global Mmlu Full Fa High School Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_fil_high_school_mathematics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Fil High School Mathematics\",\n",
      "    \"description\": \"Global Mmlu Full Fil High School Mathematics evaluation benchmark\",\n",
      "    \"category\": \"math\",\n",
      "    \"metrics\": [\n",
      "      \"exact_match\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"math\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_piqa_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Piqa Lev\",\n",
      "    \"description\": \"Aradice Piqa Lev evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1838,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_winogrande_eng\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Winogrande Eng\",\n",
      "    \"description\": \"Aradice Winogrande Eng evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1267,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_copa\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mt Copa\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mt Copa evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 500,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_copa_light\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mt Copa Light\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mt Copa Light evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 500,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_hellaswag\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mt Hellaswag\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mt Hellaswag evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 10042,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_hellaswag_light\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mt Hellaswag Light\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mt Hellaswag Light evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 10042,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_piqa\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mt Piqa\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mt Piqa evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1838,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_piqa_light\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mt Piqa Light\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mt Piqa Light evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1838,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_mt_hellaswag\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Mt Hellaswag\",\n",
      "    \"description\": \"Arabic Mt Hellaswag evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 10042,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_mt_piqa\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Mt Piqa\",\n",
      "    \"description\": \"Arabic Mt Piqa evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1838,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"copa_ar\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Copa Ar\",\n",
      "    \"description\": \"Copa Ar evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 500,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"copal_id_colloquial\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Copal Id Colloquial\",\n",
      "    \"description\": \"Copal Id Colloquial evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 500,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"darijahellaswag\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Darijahellaswag\",\n",
      "    \"description\": \"Darijahellaswag evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 10042,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"egyhellaswag\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Egyhellaswag\",\n",
      "    \"description\": \"Egyhellaswag evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 10042,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"hellaswag_ar\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Hellaswag Ar\",\n",
      "    \"description\": \"Hellaswag Ar evaluation benchmark\",\n",
      "    \"category\": \"reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 10042,\n",
      "    \"tags\": [\n",
      "      \"reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_race\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mt Race\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mt Race evaluation benchmark\",\n",
      "    \"category\": \"reading_comprehension\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 674,\n",
      "    \"tags\": [\n",
      "      \"reading_comprehension\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mt_race_light\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mt Race Light\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mt Race Light evaluation benchmark\",\n",
      "    \"category\": \"reading_comprehension\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 674,\n",
      "    \"tags\": [\n",
      "      \"reading_comprehension\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_mt_race_light\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Mt Race Light\",\n",
      "    \"description\": \"Arabic Mt Race Light evaluation benchmark\",\n",
      "    \"category\": \"reading_comprehension\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 674,\n",
      "    \"tags\": [\n",
      "      \"reading_comprehension\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"blimp_drop_argument\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Blimp Drop Argument\",\n",
      "    \"description\": \"Blimp Drop Argument evaluation benchmark\",\n",
      "    \"category\": \"reading_comprehension\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 9536,\n",
      "    \"tags\": [\n",
      "      \"reading_comprehension\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bigbench_gre_reading_comprehension_multiple_choice\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bigbench Gre Reading Comprehension Multiple Choice\",\n",
      "    \"description\": \"Bigbench Gre Reading Comprehension Multiple Choice evaluation benchmark\",\n",
      "    \"category\": \"reading_comprehension\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 3000,\n",
      "    \"tags\": [\n",
      "      \"reading_comprehension\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"eus_reading\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Eus Reading\",\n",
      "    \"description\": \"Eus Reading evaluation benchmark\",\n",
      "    \"category\": \"reading_comprehension\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 3000,\n",
      "    \"tags\": [\n",
      "      \"reading_comprehension\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"longbench_qasper\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Longbench Qasper\",\n",
      "    \"description\": \"Longbench Qasper evaluation benchmark\",\n",
      "    \"category\": \"reading_comprehension\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 3000,\n",
      "    \"tags\": [\n",
      "      \"reading_comprehension\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"qasper_freeform\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Qasper Freeform\",\n",
      "    \"description\": \"Qasper Freeform evaluation benchmark\",\n",
      "    \"category\": \"reading_comprehension\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 3000,\n",
      "    \"tags\": [\n",
      "      \"reading_comprehension\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"ruler_qa_squad\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Ruler Qa Squad\",\n",
      "    \"description\": \"Ruler Qa Squad evaluation benchmark\",\n",
      "    \"category\": \"reading_comprehension\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 3000,\n",
      "    \"tags\": [\n",
      "      \"reading_comprehension\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"scrolls_qasper\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Scrolls Qasper\",\n",
      "    \"description\": \"Scrolls Qasper evaluation benchmark\",\n",
      "    \"category\": \"reading_comprehension\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 3000,\n",
      "    \"tags\": [\n",
      "      \"reading_comprehension\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_social-science_economics_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu High Social-Science Economics Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu High Social-Science Economics Egy evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_social-science_geography_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu High Social-Science Geography Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu High Social-Science Geography Lev evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_stem_computer-science_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu High Stem Computer-Science Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu High Stem Computer-Science Egy evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_high_stem_physics_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu High Stem Physics Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu High Stem Physics Lev evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_social-science_civics_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Middle Social-Science Civics Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu Middle Social-Science Civics Egy evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_social-science_economics_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Middle Social-Science Economics Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu Middle Social-Science Economics Lev evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_social-science_social-science_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Middle Social-Science Social-Science Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu Middle Social-Science Social-Science Egy evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_middle_stem_computer-science_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Middle Stem Computer-Science Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu Middle Stem Computer-Science Lev evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_social-science_geography_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Primary Social-Science Geography Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu Primary Social-Science Geography Egy evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_social-science_social-science_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Primary Social-Science Social-Science Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu Primary Social-Science Social-Science Lev evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_primary_stem_natural-science_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Primary Stem Natural-Science Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu Primary Stem Natural-Science Lev evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_social-science_accounting_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Univ Social-Science Accounting Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu Univ Social-Science Accounting Lev evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_social-science_political-science_egy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Univ Social-Science Political-Science Egy\",\n",
      "    \"description\": \"Aradice Arabicmmlu Univ Social-Science Political-Science Egy evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_ArabicMMLU_univ_stem_computer-science_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Arabicmmlu Univ Stem Computer-Science Lev\",\n",
      "    \"description\": \"Aradice Arabicmmlu Univ Stem Computer-Science Lev evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_college_biology_light\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mmlu College Biology Light\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mmlu College Biology Light evaluation benchmark\",\n",
      "    \"category\": \"science\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"science\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"agieval_logiqa_zh\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Agieval Logiqa Zh\",\n",
      "    \"description\": \"Agieval Logiqa Zh evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 651,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh\",\n",
      "    \"description\": \"Bbh evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_fewshot\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Fewshot\",\n",
      "    \"description\": \"Bbh Cot Fewshot evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 5,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_fewshot_causal_judgement\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Fewshot Causal Judgement\",\n",
      "    \"description\": \"Bbh Cot Fewshot Causal Judgement evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 5,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_fewshot_dyck_languages\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Fewshot Dyck Languages\",\n",
      "    \"description\": \"Bbh Cot Fewshot Dyck Languages evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 5,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_fewshot_hyperbaton\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Fewshot Hyperbaton\",\n",
      "    \"description\": \"Bbh Cot Fewshot Hyperbaton evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 5,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_fewshot_logical_deduction_three_objects\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Fewshot Logical Deduction Three Objects\",\n",
      "    \"description\": \"Bbh Cot Fewshot Logical Deduction Three Objects evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 5,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_fewshot_navigate\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Fewshot Navigate\",\n",
      "    \"description\": \"Bbh Cot Fewshot Navigate evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 5,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_fewshot_reasoning_about_colored_objects\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Fewshot Reasoning About Colored Objects\",\n",
      "    \"description\": \"Bbh Cot Fewshot Reasoning About Colored Objects evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 5,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_fewshot_snarks\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Fewshot Snarks\",\n",
      "    \"description\": \"Bbh Cot Fewshot Snarks evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 5,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_fewshot_tracking_shuffled_objects_five_objects\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Fewshot Tracking Shuffled Objects Five Objects\",\n",
      "    \"description\": \"Bbh Cot Fewshot Tracking Shuffled Objects Five Objects evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 5,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_fewshot_web_of_lies\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Fewshot Web Of Lies\",\n",
      "    \"description\": \"Bbh Cot Fewshot Web Of Lies evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 5,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_zeroshot\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Zeroshot\",\n",
      "    \"description\": \"Bbh Cot Zeroshot evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_zeroshot_causal_judgement\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Zeroshot Causal Judgement\",\n",
      "    \"description\": \"Bbh Cot Zeroshot Causal Judgement evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bbh_cot_zeroshot_dyck_languages\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bbh Cot Zeroshot Dyck Languages\",\n",
      "    \"description\": \"Bbh Cot Zeroshot Dyck Languages evaluation benchmark\",\n",
      "    \"category\": \"logic_reasoning\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"logic_reasoning\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_anatomy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mmlu Anatomy\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mmlu Anatomy evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_clinical_knowledge\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mmlu Clinical Knowledge\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mmlu Clinical Knowledge evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_medical_genetics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mmlu Medical Genetics\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mmlu Medical Genetics evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"arabic_leaderboard_arabic_mmlu_professional_medicine\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Arabic Leaderboard Arabic Mmlu Professional Medicine\",\n",
      "    \"description\": \"Arabic Leaderboard Arabic Mmlu Professional Medicine evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"cmmlu_professional_medicine\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Cmmlu Professional Medicine\",\n",
      "    \"description\": \"Cmmlu Professional Medicine evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"cmmlu_traditional_chinese_medicine\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Cmmlu Traditional Chinese Medicine\",\n",
      "    \"description\": \"Cmmlu Traditional Chinese Medicine evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_am_anatomy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Am Anatomy\",\n",
      "    \"description\": \"Global Mmlu Full Am Anatomy evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_am_clinical_knowledge\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Am Clinical Knowledge\",\n",
      "    \"description\": \"Global Mmlu Full Am Clinical Knowledge evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_am_medical_genetics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Am Medical Genetics\",\n",
      "    \"description\": \"Global Mmlu Full Am Medical Genetics evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_am_professional_medicine\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Am Professional Medicine\",\n",
      "    \"description\": \"Global Mmlu Full Am Professional Medicine evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_ar_anatomy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Ar Anatomy\",\n",
      "    \"description\": \"Global Mmlu Full Ar Anatomy evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_ar_clinical_knowledge\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Ar Clinical Knowledge\",\n",
      "    \"description\": \"Global Mmlu Full Ar Clinical Knowledge evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_ar_medical_genetics\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Ar Medical Genetics\",\n",
      "    \"description\": \"Global Mmlu Full Ar Medical Genetics evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_ar_professional_medicine\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Ar Professional Medicine\",\n",
      "    \"description\": \"Global Mmlu Full Ar Professional Medicine evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"global_mmlu_full_bn_anatomy\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Global Mmlu Full Bn Anatomy\",\n",
      "    \"description\": \"Global Mmlu Full Bn Anatomy evaluation benchmark\",\n",
      "    \"category\": \"medical\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 14042,\n",
      "    \"tags\": [\n",
      "      \"medical\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"lambada_openai\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Lambada Openai\",\n",
      "    \"description\": \"Lambada Openai evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"perplexity\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 5153,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"lambada_openai_mt_en\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Lambada Openai Mt En\",\n",
      "    \"description\": \"Lambada Openai Mt En evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"perplexity\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 5153,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"lambada_openai_mt_it\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Lambada Openai Mt It\",\n",
      "    \"description\": \"Lambada Openai Mt It evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"perplexity\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 5153,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"lambada_openai_mt_stablelm_es\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Lambada Openai Mt Stablelm Es\",\n",
      "    \"description\": \"Lambada Openai Mt Stablelm Es evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"perplexity\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 5153,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"lambada_openai_mt_stablelm_nl\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Lambada Openai Mt Stablelm Nl\",\n",
      "    \"description\": \"Lambada Openai Mt Stablelm Nl evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"perplexity\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 5153,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"lambada_standard_cloze_yaml\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Lambada Standard Cloze Yaml\",\n",
      "    \"description\": \"Lambada Standard Cloze Yaml evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"perplexity\",\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 5153,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"paloma_wikitext_103\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Paloma Wikitext 103\",\n",
      "    \"description\": \"Paloma Wikitext 103 evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 4358,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"pile_arxiv\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Pile Arxiv\",\n",
      "    \"description\": \"Pile Arxiv evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 210000000,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"pile_freelaw\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Pile Freelaw\",\n",
      "    \"description\": \"Pile Freelaw evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 210000000,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"pile_hackernews\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Pile Hackernews\",\n",
      "    \"description\": \"Pile Hackernews evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 210000000,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"pile_openwebtext2\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Pile Openwebtext2\",\n",
      "    \"description\": \"Pile Openwebtext2 evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 210000000,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"pile_ubuntu-irc\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Pile Ubuntu-Irc\",\n",
      "    \"description\": \"Pile Ubuntu-Irc evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 210000000,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"pile_youtubesubtitles\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Pile Youtubesubtitles\",\n",
      "    \"description\": \"Pile Youtubesubtitles evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 210000000,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"wikitext\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Wikitext\",\n",
      "    \"description\": \"Wikitext evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 4358,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"careqa_open_perplexity\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Careqa Open Perplexity\",\n",
      "    \"description\": \"Careqa Open Perplexity evaluation benchmark\",\n",
      "    \"category\": \"language_modeling\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 10000,\n",
      "    \"tags\": [\n",
      "      \"language_modeling\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"AraDiCE_truthfulqa_mc1_lev\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Aradice Truthfulqa Mc1 Lev\",\n",
      "    \"description\": \"Aradice Truthfulqa Mc1 Lev evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"metabench_truthfulqa_permute\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Metabench Truthfulqa Permute\",\n",
      "    \"description\": \"Metabench Truthfulqa Permute evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"nortruthfulqa_gen_nno_p0\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Nortruthfulqa Gen Nno P0\",\n",
      "    \"description\": \"Nortruthfulqa Gen Nno P0 evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"nortruthfulqa_gen_nno_p3\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Nortruthfulqa Gen Nno P3\",\n",
      "    \"description\": \"Nortruthfulqa Gen Nno P3 evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"nortruthfulqa_gen_nob_p1\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Nortruthfulqa Gen Nob P1\",\n",
      "    \"description\": \"Nortruthfulqa Gen Nob P1 evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"nortruthfulqa_gen_nob_p4\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Nortruthfulqa Gen Nob P4\",\n",
      "    \"description\": \"Nortruthfulqa Gen Nob P4 evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"nortruthfulqa_mc_nno_p2\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Nortruthfulqa Mc Nno P2\",\n",
      "    \"description\": \"Nortruthfulqa Mc Nno P2 evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"nortruthfulqa_mc_nob_p0\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Nortruthfulqa Mc Nob P0\",\n",
      "    \"description\": \"Nortruthfulqa Mc Nob P0 evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"nortruthfulqa_mc_nob_p3\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Nortruthfulqa Mc Nob P3\",\n",
      "    \"description\": \"Nortruthfulqa Mc Nob P3 evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"tinyTruthfulQA\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Tinytruthfulqa\",\n",
      "    \"description\": \"Tinytruthfulqa evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"truthfulqa-multi_gen_ca\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Truthfulqa-Multi Gen Ca\",\n",
      "    \"description\": \"Truthfulqa-Multi Gen Ca evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"truthfulqa-multi_gen_eu\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Truthfulqa-Multi Gen Eu\",\n",
      "    \"description\": \"Truthfulqa-Multi Gen Eu evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"mc1\",\n",
      "      \"mc2\",\n",
      "      \"bleu\",\n",
      "      \"rouge\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"truthfulqa-multi_mc1_en\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Truthfulqa-Multi Mc1 En\",\n",
      "    \"description\": \"Truthfulqa-Multi Mc1 En evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"truthfulqa-multi_mc1_gl\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Truthfulqa-Multi Mc1 Gl\",\n",
      "    \"description\": \"Truthfulqa-Multi Mc1 Gl evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"truthfulqa-multi_mc2_es\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Truthfulqa-Multi Mc2 Es\",\n",
      "    \"description\": \"Truthfulqa-Multi Mc2 Es evaluation benchmark\",\n",
      "    \"category\": \"safety\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 817,\n",
      "    \"tags\": [\n",
      "      \"safety\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"bigbench_code_line_description_multiple_choice\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Bigbench Code Line Description Multiple Choice\",\n",
      "    \"description\": \"Bigbench Code Line Description Multiple Choice evaluation benchmark\",\n",
      "    \"category\": \"code\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\",\n",
      "      \"acc_norm\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"code\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"ceval-valid_college_programming\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Ceval-Valid College Programming\",\n",
      "    \"description\": \"Ceval-Valid College Programming evaluation benchmark\",\n",
      "    \"category\": \"code\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"code\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"code2text_javascript\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Code2Text Javascript\",\n",
      "    \"description\": \"Code2Text Javascript evaluation benchmark\",\n",
      "    \"category\": \"code\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"code\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"code2text_ruby\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Code2Text Ruby\",\n",
      "    \"description\": \"Code2Text Ruby evaluation benchmark\",\n",
      "    \"category\": \"code\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"code\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"humaneval\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Humaneval\",\n",
      "    \"description\": \"Humaneval evaluation benchmark\",\n",
      "    \"category\": \"code\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"code\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"humaneval_instruct\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"Humaneval Instruct\",\n",
      "    \"description\": \"Humaneval Instruct evaluation benchmark\",\n",
      "    \"category\": \"code\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"code\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  },\n",
      "  {\n",
      "    \"benchmark_id\": \"mbpp\",\n",
      "    \"provider_id\": \"lm_evaluation_harness\",\n",
      "    \"provider_name\": \"LM Evaluation Harness\",\n",
      "    \"name\": \"MBPP\",\n",
      "    \"description\": \"MBPP (Most Basic Python Programming) evaluation benchmark\",\n",
      "    \"category\": \"code\",\n",
      "    \"metrics\": [\n",
      "      \"accuracy\"\n",
      "    ],\n",
      "    \"num_few_shot\": 0,\n",
      "    \"dataset_size\": 1000,\n",
      "    \"tags\": [\n",
      "      \"code\",\n",
      "      \"lm_eval\"\n",
      "    ],\n",
      "    \"provider_type\": \"builtin\"\n",
      "  }\n",
      "]\n",
      "--------------------------------------------------\n",
      "Benchmarks for lm_evaluation_harness: 168\n",
      "\n",
      "Reasoning: 16 benchmarks\n",
      "  Examples: ARC Easy, Aradice Piqa Lev, Aradice Winogrande Eng\n",
      "\n",
      "General: 15 benchmarks\n",
      "  Examples: Aradice Boolq Lev, Blimp, Blimp Anaphor Gender Agreement\n",
      "\n",
      "Knowledge: 15 benchmarks\n",
      "  Examples: Aradice Arabicmmlu Egy, Aradice Arabicmmlu High Humanities History Lev, Aradice Arabicmmlu High Humanities Philosophy Egy\n",
      "\n",
      "Multilingual: 15 benchmarks\n",
      "  Examples: Arabic Leaderboard Arabic Mt Boolq, Arabic Leaderboard Arabic Mt Boolq Light, Arabic Mt Boolq Light\n",
      "\n",
      "Math: 15 benchmarks\n",
      "  Examples: Aradice Arabicmmlu Primary Stem Math Egy, Arabic Leaderboard Arabic Mmlu College Mathematics Light, Arabic Leaderboard Arabic Mmlu High School Mathematics\n",
      "\n",
      "Reading_Comprehension: 10 benchmarks\n",
      "  Examples: Arabic Leaderboard Arabic Mt Race, Arabic Leaderboard Arabic Mt Race Light, Arabic Mt Race Light\n",
      "\n",
      "Science: 15 benchmarks\n",
      "  Examples: Aradice Arabicmmlu High Social-Science Economics Egy, Aradice Arabicmmlu High Social-Science Geography Lev, Aradice Arabicmmlu High Stem Computer-Science Egy\n",
      "\n",
      "Logic_Reasoning: 15 benchmarks\n",
      "  Examples: Agieval Logiqa Zh, Bbh, Bbh Cot Fewshot\n",
      "\n",
      "Medical: 15 benchmarks\n",
      "  Examples: Arabic Leaderboard Arabic Mmlu Anatomy, Arabic Leaderboard Arabic Mmlu Clinical Knowledge, Arabic Leaderboard Arabic Mmlu Medical Genetics\n",
      "\n",
      "Language_Modeling: 15 benchmarks\n",
      "  Examples: Lambada Openai, Lambada Openai Mt En, Lambada Openai Mt It\n",
      "\n",
      "Safety: 15 benchmarks\n",
      "  Examples: Aradice Truthfulqa Mc1 Lev, Metabench Truthfulqa Permute, Nortruthfulqa Gen Nno P0\n",
      "\n",
      "Code: 7 benchmarks\n",
      "  Examples: Bigbench Code Line Description Multiple Choice, Ceval-Valid College Programming, Code2Text Javascript\n"
     ]
    }
   ],
   "source": [
    "provider_id = \"lm_evaluation_harness\"\n",
    "response = api_request(\"GET\", f\"/providers/{provider_id}/benchmarks\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    benchmarks = response.json()\n",
    "    print(f\"Benchmarks for {provider_id}: {len(benchmarks)}\")\n",
    "\n",
    "    # Group by category\n",
    "    categories = {}\n",
    "    for benchmark in benchmarks:\n",
    "        category = benchmark['category']\n",
    "        if category not in categories:\n",
    "            categories[category] = []\n",
    "        categories[category].append(benchmark['name'])\n",
    "\n",
    "    for category, names in categories.items():\n",
    "        print(f\"\\n{category.title()}: {len(names)} benchmarks\")\n",
    "        print(f\"  Examples: {', '.join(names[:3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections\n",
    "\n",
    "### List Available Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://localhost:8000/api/v1/collections\n",
      "Status: 200\n",
      "Response:\n",
      "{\n",
      "  \"collections\": [\n",
      "    {\n",
      "      \"collection_id\": \"healthcare_safety_v1\",\n",
      "      \"name\": \"Healthcare Safety Collection v1\",\n",
      "      \"description\": \"Comprehensive healthcare AI safety evaluation suite\",\n",
      "      \"provider_id\": null,\n",
      "      \"tags\": [],\n",
      "      \"metadata\": {},\n",
      "      \"benchmarks\": [\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"truthfulqa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"pubmedqa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"medmcqa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"garak\",\n",
      "          \"benchmark_id\": \"bias_detection\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"garak\",\n",
      "          \"benchmark_id\": \"pii_leakage\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        }\n",
      "      ],\n",
      "      \"created_at\": null,\n",
      "      \"updated_at\": null\n",
      "    },\n",
      "    {\n",
      "      \"collection_id\": \"automotive_safety_v1\",\n",
      "      \"name\": \"Automotive Safety Collection v1\",\n",
      "      \"description\": \"Automotive AI safety and reliability evaluation suite\",\n",
      "      \"provider_id\": null,\n",
      "      \"tags\": [],\n",
      "      \"metadata\": {},\n",
      "      \"benchmarks\": [\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"hellaswag\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"arc_challenge\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"commonsense_qa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"garak\",\n",
      "          \"benchmark_id\": \"toxicity\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        }\n",
      "      ],\n",
      "      \"created_at\": null,\n",
      "      \"updated_at\": null\n",
      "    },\n",
      "    {\n",
      "      \"collection_id\": \"finance_compliance_v1\",\n",
      "      \"name\": \"Financial Compliance Collection v1\",\n",
      "      \"description\": \"Financial AI compliance and accuracy evaluation suite\",\n",
      "      \"provider_id\": null,\n",
      "      \"tags\": [],\n",
      "      \"metadata\": {},\n",
      "      \"benchmarks\": [\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"gsm8k\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"mathqa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"truthfulqa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"garak\",\n",
      "          \"benchmark_id\": \"pii_leakage\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        }\n",
      "      ],\n",
      "      \"created_at\": null,\n",
      "      \"updated_at\": null\n",
      "    },\n",
      "    {\n",
      "      \"collection_id\": \"general_llm_eval_v1\",\n",
      "      \"name\": \"General LLM Evaluation v1\",\n",
      "      \"description\": \"Comprehensive general-purpose LLM evaluation suite\",\n",
      "      \"provider_id\": null,\n",
      "      \"tags\": [],\n",
      "      \"metadata\": {},\n",
      "      \"benchmarks\": [\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"mmlu\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"hellaswag\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"arc_challenge\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"truthfulqa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"gsm8k\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"winogrande\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        }\n",
      "      ],\n",
      "      \"created_at\": null,\n",
      "      \"updated_at\": null\n",
      "    }\n",
      "  ],\n",
      "  \"total_collections\": 4\n",
      "}\n",
      "--------------------------------------------------\n",
      "Available collections: 4\n",
      "\n",
      "📁 Healthcare Safety Collection v1 (healthcare_safety_v1)\n",
      "   Description: Comprehensive healthcare AI safety evaluation suite\n",
      "   Benchmarks: 5\n",
      "     - lm_evaluation_harness::truthfulqa\n",
      "     - lm_evaluation_harness::pubmedqa\n",
      "     - lm_evaluation_harness::medmcqa\n",
      "\n",
      "📁 Automotive Safety Collection v1 (automotive_safety_v1)\n",
      "   Description: Automotive AI safety and reliability evaluation suite\n",
      "   Benchmarks: 4\n",
      "     - lm_evaluation_harness::hellaswag\n",
      "     - lm_evaluation_harness::arc_challenge\n",
      "     - lm_evaluation_harness::commonsense_qa\n",
      "\n",
      "📁 Financial Compliance Collection v1 (finance_compliance_v1)\n",
      "   Description: Financial AI compliance and accuracy evaluation suite\n",
      "   Benchmarks: 4\n",
      "     - lm_evaluation_harness::gsm8k\n",
      "     - lm_evaluation_harness::mathqa\n",
      "     - lm_evaluation_harness::truthfulqa\n",
      "\n",
      "📁 General LLM Evaluation v1 (general_llm_eval_v1)\n",
      "   Description: Comprehensive general-purpose LLM evaluation suite\n",
      "   Benchmarks: 6\n",
      "     - lm_evaluation_harness::mmlu\n",
      "     - lm_evaluation_harness::hellaswag\n",
      "     - lm_evaluation_harness::arc_challenge\n"
     ]
    }
   ],
   "source": [
    "response = api_request(\"GET\", \"/collections\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    collections = response.json()\n",
    "    print(f\"Available collections: {collections['total_collections']}\")\n",
    "\n",
    "    for collection in collections['collections']:\n",
    "        print(f\"\\n📁 {collection['name']} ({collection['collection_id']})\")\n",
    "        print(f\"   Description: {collection['description']}\")\n",
    "        print(f\"   Benchmarks: {len(collection['benchmarks'])}\")\n",
    "        for benchmark_ref in collection['benchmarks'][:3]:  # Show first 3\n",
    "            print(f\"     - {benchmark_ref['provider_id']}::{benchmark_ref['benchmark_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Custom Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Creating custom coding & reasoning collection...\n",
      "{\n",
      "  \"collection_id\": \"coding_reasoning_v1\",\n",
      "  \"name\": \"Coding & Reasoning Collection v1\",\n",
      "  \"description\": \"A curated collection of coding and reasoning benchmarks using available lm-evaluation-harness tasks\",\n",
      "  \"tags\": [\n",
      "    \"coding\",\n",
      "    \"reasoning\",\n",
      "    \"v1\"\n",
      "  ],\n",
      "  \"benchmarks\": [\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"arc_easy\",\n",
      "      \"weight\": 1.5,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 25,\n",
      "        \"limit\": 100\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"humaneval\",\n",
      "      \"weight\": 2.0,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 0,\n",
      "        \"limit\": 50\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"mbpp\",\n",
      "      \"weight\": 2.0,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 0,\n",
      "        \"limit\": 50\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"bbh\",\n",
      "      \"weight\": 1.5,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 3,\n",
      "        \"limit\": 100\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"created_by\": \"evaluation_team\",\n",
      "    \"use_case\": \"coding_reasoning_assessment\",\n",
      "    \"difficulty\": \"intermediate_to_hard\",\n",
      "    \"estimated_duration_minutes\": 30\n",
      "  }\n",
      "}\n",
      "POST http://localhost:8000/api/v1/collections\n",
      "Status: 201\n",
      "Response:\n",
      "{\n",
      "  \"collection_id\": \"coding_reasoning_v1\",\n",
      "  \"name\": \"Coding & Reasoning Collection v1\",\n",
      "  \"description\": \"A curated collection of coding and reasoning benchmarks using available lm-evaluation-harness tasks\",\n",
      "  \"provider_id\": null,\n",
      "  \"tags\": [\n",
      "    \"coding\",\n",
      "    \"reasoning\",\n",
      "    \"v1\"\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"created_by\": \"evaluation_team\",\n",
      "    \"use_case\": \"coding_reasoning_assessment\",\n",
      "    \"difficulty\": \"intermediate_to_hard\",\n",
      "    \"estimated_duration_minutes\": 30\n",
      "  },\n",
      "  \"benchmarks\": [\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"arc_easy\",\n",
      "      \"weight\": 1.5,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 25,\n",
      "        \"limit\": 100\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"humaneval\",\n",
      "      \"weight\": 2.0,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 0,\n",
      "        \"limit\": 50\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"mbpp\",\n",
      "      \"weight\": 2.0,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 0,\n",
      "        \"limit\": 50\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"bbh\",\n",
      "      \"weight\": 1.5,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 3,\n",
      "        \"limit\": 100\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created_at\": \"2025-11-09T23:48:17.846931Z\",\n",
      "  \"updated_at\": \"2025-11-09T23:48:17.846931Z\"\n",
      "}\n",
      "--------------------------------------------------\n",
      "✅ Collection created successfully!\n",
      "Collection ID: coding_reasoning_v1\n",
      "Total benchmarks: 4\n",
      "Created at: 2025-11-09T23:48:17.846931Z\n"
     ]
    }
   ],
   "source": [
    "# Create a collection of available lm-evaluation-harness benchmarks for coding and reasoning evaluation\n",
    "coding_reasoning_collection = {\n",
    "    \"collection_id\": \"coding_reasoning_v1\",\n",
    "    \"name\": \"Coding & Reasoning Collection v1\",\n",
    "    \"description\": \"A curated collection of coding and reasoning benchmarks using available lm-evaluation-harness tasks\",\n",
    "    \"tags\": [\"coding\", \"reasoning\", \"v1\"],\n",
    "    \"benchmarks\": [\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"arc_easy\",\n",
    "            \"weight\": 1.5,\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 25,\n",
    "                \"limit\": 100\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"humaneval\",\n",
    "            \"weight\": 2.0,  # Higher weight for coding benchmark\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 0,\n",
    "                \"limit\": 50\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"mbpp\",\n",
    "            \"weight\": 2.0,\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 0,\n",
    "                \"limit\": 50\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"bbh\",\n",
    "            \"weight\": 1.5,  # Big-bench hard for reasoning\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 3,\n",
    "                \"limit\": 100\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"created_by\": \"evaluation_team\",\n",
    "        \"use_case\": \"coding_reasoning_assessment\",\n",
    "        \"difficulty\": \"intermediate_to_hard\",\n",
    "        \"estimated_duration_minutes\": 30\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📝 Creating custom coding & reasoning collection...\")\n",
    "print_json(coding_reasoning_collection)\n",
    "\n",
    "response = api_request(\"POST\", \"/collections\", json=coding_reasoning_collection)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    created_collection = response.json()\n",
    "    print(\"✅ Collection created successfully!\")\n",
    "    print(f\"Collection ID: {created_collection['collection_id']}\")\n",
    "    print(f\"Total benchmarks: {len(created_collection['benchmarks'])}\")\n",
    "    print(f\"Created at: {created_collection.get('created_at', 'N/A')}\")\n",
    "\n",
    "    # Store collection ID for later use\n",
    "    coding_reasoning_collection_id = created_collection['collection_id']\n",
    "else:\n",
    "    print(f\"❌ Failed to create collection: {response.text}\")\n",
    "    coding_reasoning_collection_id = \"coding_reasoning_v1\"  # Fallback for examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Language Understanding Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Creating language understanding collection...\n",
      "POST http://localhost:8000/api/v1/collections\n",
      "Status: 201\n",
      "Response:\n",
      "{\n",
      "  \"collection_id\": \"language_understanding_v1\",\n",
      "  \"name\": \"Language Understanding Collection v1\",\n",
      "  \"description\": \"Collection of language modeling and comprehension benchmarks\",\n",
      "  \"provider_id\": null,\n",
      "  \"tags\": [\n",
      "    \"language\",\n",
      "    \"understanding\",\n",
      "    \"comprehension\"\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"created_by\": \"nlp_team\",\n",
      "    \"use_case\": \"language_understanding_assessment\",\n",
      "    \"difficulty\": \"beginner_to_intermediate\",\n",
      "    \"focus_areas\": [\n",
      "      \"language_modeling\",\n",
      "      \"grammar\",\n",
      "      \"comprehension\"\n",
      "    ]\n",
      "  },\n",
      "  \"benchmarks\": [\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"lambada_openai\",\n",
      "      \"weight\": 1.0,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 0,\n",
      "        \"limit\": 200\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"blimp\",\n",
      "      \"weight\": 1.5,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 0,\n",
      "        \"limit\": 100\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"arc_easy\",\n",
      "      \"weight\": 1.0,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 25,\n",
      "        \"limit\": 100\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created_at\": \"2025-11-09T23:48:18.948667Z\",\n",
      "  \"updated_at\": \"2025-11-09T23:48:18.948667Z\"\n",
      "}\n",
      "--------------------------------------------------\n",
      "✅ Language collection created: language_understanding_v1\n"
     ]
    }
   ],
   "source": [
    "# Create a collection focused on language understanding and modeling\n",
    "language_collection = {\n",
    "    \"collection_id\": \"language_understanding_v1\",\n",
    "    \"name\": \"Language Understanding Collection v1\",\n",
    "    \"description\": \"Collection of language modeling and comprehension benchmarks\",\n",
    "    \"tags\": [\"language\", \"understanding\", \"comprehension\"],\n",
    "    \"benchmarks\": [\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"lambada_openai\",\n",
    "            \"weight\": 1.0,\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 0,\n",
    "                \"limit\": 200\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"blimp\",\n",
    "            \"weight\": 1.5,  # Grammar and linguistic knowledge\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 0,\n",
    "                \"limit\": 100\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"provider_id\": \"lm_evaluation_harness\",\n",
    "            \"benchmark_id\": \"arc_easy\",  # For basic reasoning\n",
    "            \"weight\": 1.0,\n",
    "            \"config\": {\n",
    "                \"num_fewshot\": 25,\n",
    "                \"limit\": 100\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"created_by\": \"nlp_team\",\n",
    "        \"use_case\": \"language_understanding_assessment\",\n",
    "        \"difficulty\": \"beginner_to_intermediate\",\n",
    "        \"focus_areas\": [\"language_modeling\", \"grammar\", \"comprehension\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📝 Creating language understanding collection...\")\n",
    "response = api_request(\"POST\", \"/collections\", json=language_collection)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    language_collection_id = response.json()['collection_id']\n",
    "    print(f\"✅ Language collection created: {language_collection_id}\")\n",
    "else:\n",
    "    print(\"⚠️ Language collection creation failed (may already exist)\")\n",
    "    language_collection_id = \"language_understanding_v1\"  # Fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Collections (Including New Ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://localhost:8000/api/v1/collections\n",
      "Status: 200\n",
      "Response:\n",
      "{\n",
      "  \"collections\": [\n",
      "    {\n",
      "      \"collection_id\": \"healthcare_safety_v1\",\n",
      "      \"name\": \"Healthcare Safety Collection v1\",\n",
      "      \"description\": \"Comprehensive healthcare AI safety evaluation suite\",\n",
      "      \"provider_id\": null,\n",
      "      \"tags\": [],\n",
      "      \"metadata\": {},\n",
      "      \"benchmarks\": [\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"truthfulqa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"pubmedqa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"medmcqa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"garak\",\n",
      "          \"benchmark_id\": \"bias_detection\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"garak\",\n",
      "          \"benchmark_id\": \"pii_leakage\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        }\n",
      "      ],\n",
      "      \"created_at\": null,\n",
      "      \"updated_at\": null\n",
      "    },\n",
      "    {\n",
      "      \"collection_id\": \"automotive_safety_v1\",\n",
      "      \"name\": \"Automotive Safety Collection v1\",\n",
      "      \"description\": \"Automotive AI safety and reliability evaluation suite\",\n",
      "      \"provider_id\": null,\n",
      "      \"tags\": [],\n",
      "      \"metadata\": {},\n",
      "      \"benchmarks\": [\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"hellaswag\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"arc_challenge\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"commonsense_qa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"garak\",\n",
      "          \"benchmark_id\": \"toxicity\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        }\n",
      "      ],\n",
      "      \"created_at\": null,\n",
      "      \"updated_at\": null\n",
      "    },\n",
      "    {\n",
      "      \"collection_id\": \"finance_compliance_v1\",\n",
      "      \"name\": \"Financial Compliance Collection v1\",\n",
      "      \"description\": \"Financial AI compliance and accuracy evaluation suite\",\n",
      "      \"provider_id\": null,\n",
      "      \"tags\": [],\n",
      "      \"metadata\": {},\n",
      "      \"benchmarks\": [\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"gsm8k\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"mathqa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"truthfulqa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"garak\",\n",
      "          \"benchmark_id\": \"pii_leakage\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        }\n",
      "      ],\n",
      "      \"created_at\": null,\n",
      "      \"updated_at\": null\n",
      "    },\n",
      "    {\n",
      "      \"collection_id\": \"general_llm_eval_v1\",\n",
      "      \"name\": \"General LLM Evaluation v1\",\n",
      "      \"description\": \"Comprehensive general-purpose LLM evaluation suite\",\n",
      "      \"provider_id\": null,\n",
      "      \"tags\": [],\n",
      "      \"metadata\": {},\n",
      "      \"benchmarks\": [\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"mmlu\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"hellaswag\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"arc_challenge\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"truthfulqa\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"gsm8k\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"winogrande\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {}\n",
      "        }\n",
      "      ],\n",
      "      \"created_at\": null,\n",
      "      \"updated_at\": null\n",
      "    },\n",
      "    {\n",
      "      \"collection_id\": \"coding_reasoning_v1\",\n",
      "      \"name\": \"Coding & Reasoning Collection v1\",\n",
      "      \"description\": \"A curated collection of coding and reasoning benchmarks using available lm-evaluation-harness tasks\",\n",
      "      \"provider_id\": null,\n",
      "      \"tags\": [\n",
      "        \"coding\",\n",
      "        \"reasoning\",\n",
      "        \"v1\"\n",
      "      ],\n",
      "      \"metadata\": {\n",
      "        \"created_by\": \"evaluation_team\",\n",
      "        \"use_case\": \"coding_reasoning_assessment\",\n",
      "        \"difficulty\": \"intermediate_to_hard\",\n",
      "        \"estimated_duration_minutes\": 30\n",
      "      },\n",
      "      \"benchmarks\": [\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"arc_easy\",\n",
      "          \"weight\": 1.5,\n",
      "          \"config\": {\n",
      "            \"num_fewshot\": 25,\n",
      "            \"limit\": 100\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"humaneval\",\n",
      "          \"weight\": 2.0,\n",
      "          \"config\": {\n",
      "            \"num_fewshot\": 0,\n",
      "            \"limit\": 50\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"mbpp\",\n",
      "          \"weight\": 2.0,\n",
      "          \"config\": {\n",
      "            \"num_fewshot\": 0,\n",
      "            \"limit\": 50\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"bbh\",\n",
      "          \"weight\": 1.5,\n",
      "          \"config\": {\n",
      "            \"num_fewshot\": 3,\n",
      "            \"limit\": 100\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"created_at\": \"2025-11-09T23:48:17.846931Z\",\n",
      "      \"updated_at\": \"2025-11-09T23:48:17.846931Z\"\n",
      "    },\n",
      "    {\n",
      "      \"collection_id\": \"language_understanding_v1\",\n",
      "      \"name\": \"Language Understanding Collection v1\",\n",
      "      \"description\": \"Collection of language modeling and comprehension benchmarks\",\n",
      "      \"provider_id\": null,\n",
      "      \"tags\": [\n",
      "        \"language\",\n",
      "        \"understanding\",\n",
      "        \"comprehension\"\n",
      "      ],\n",
      "      \"metadata\": {\n",
      "        \"created_by\": \"nlp_team\",\n",
      "        \"use_case\": \"language_understanding_assessment\",\n",
      "        \"difficulty\": \"beginner_to_intermediate\",\n",
      "        \"focus_areas\": [\n",
      "          \"language_modeling\",\n",
      "          \"grammar\",\n",
      "          \"comprehension\"\n",
      "        ]\n",
      "      },\n",
      "      \"benchmarks\": [\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"lambada_openai\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {\n",
      "            \"num_fewshot\": 0,\n",
      "            \"limit\": 200\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"blimp\",\n",
      "          \"weight\": 1.5,\n",
      "          \"config\": {\n",
      "            \"num_fewshot\": 0,\n",
      "            \"limit\": 100\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"provider_id\": \"lm_evaluation_harness\",\n",
      "          \"benchmark_id\": \"arc_easy\",\n",
      "          \"weight\": 1.0,\n",
      "          \"config\": {\n",
      "            \"num_fewshot\": 25,\n",
      "            \"limit\": 100\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"created_at\": \"2025-11-09T23:48:18.948667Z\",\n",
      "      \"updated_at\": \"2025-11-09T23:48:18.948667Z\"\n",
      "    }\n",
      "  ],\n",
      "  \"total_collections\": 6\n",
      "}\n",
      "--------------------------------------------------\n",
      "📁 Total collections available: 6\n",
      "\n",
      "📁 Healthcare Safety Collection v1\n",
      "   ID: healthcare_safety_v1\n",
      "   Provider: None\n",
      "   Benchmarks: 5\n",
      "   Tags: \n",
      "\n",
      "📁 Automotive Safety Collection v1\n",
      "   ID: automotive_safety_v1\n",
      "   Provider: None\n",
      "   Benchmarks: 4\n",
      "   Tags: \n",
      "\n",
      "📁 Financial Compliance Collection v1\n",
      "   ID: finance_compliance_v1\n",
      "   Provider: None\n",
      "   Benchmarks: 4\n",
      "   Tags: \n",
      "\n",
      "📁 General LLM Evaluation v1\n",
      "   ID: general_llm_eval_v1\n",
      "   Provider: None\n",
      "   Benchmarks: 6\n",
      "   Tags: \n",
      "\n",
      "📁 Coding & Reasoning Collection v1\n",
      "   ID: coding_reasoning_v1\n",
      "   Provider: None\n",
      "   Benchmarks: 4\n",
      "   Tags: coding, reasoning, v1\n",
      "   Difficulty: intermediate_to_hard\n",
      "\n",
      "📁 Language Understanding Collection v1\n",
      "   ID: language_understanding_v1\n",
      "   Provider: None\n",
      "   Benchmarks: 3\n",
      "   Tags: language, understanding, comprehension\n",
      "   Difficulty: beginner_to_intermediate\n"
     ]
    }
   ],
   "source": [
    "# Refresh the collections list to see our new collections\n",
    "response = api_request(\"GET\", \"/collections\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    collections = response.json()\n",
    "    print(f\"📁 Total collections available: {collections['total_collections']}\")\n",
    "\n",
    "    # Show all collections with details\n",
    "    for collection in collections['collections']:\n",
    "        print(f\"\\n📁 {collection['name']}\")\n",
    "        print(f\"   ID: {collection['collection_id']}\")\n",
    "        print(f\"   Provider: {collection['provider_id']}\")\n",
    "        print(f\"   Benchmarks: {len(collection['benchmarks'])}\")\n",
    "        print(f\"   Tags: {', '.join(collection.get('tags', []))}\")\n",
    "        if collection.get('metadata', {}).get('difficulty'):\n",
    "            print(f\"   Difficulty: {collection['metadata']['difficulty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Specific Collection Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET http://localhost:8000/api/v1/collections/coding_reasoning_v1\n",
      "Status: 200\n",
      "Response:\n",
      "{\n",
      "  \"collection_id\": \"coding_reasoning_v1\",\n",
      "  \"name\": \"Coding & Reasoning Collection v1\",\n",
      "  \"description\": \"A curated collection of coding and reasoning benchmarks using available lm-evaluation-harness tasks\",\n",
      "  \"provider_id\": null,\n",
      "  \"tags\": [\n",
      "    \"coding\",\n",
      "    \"reasoning\",\n",
      "    \"v1\"\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"created_by\": \"evaluation_team\",\n",
      "    \"use_case\": \"coding_reasoning_assessment\",\n",
      "    \"difficulty\": \"intermediate_to_hard\",\n",
      "    \"estimated_duration_minutes\": 30\n",
      "  },\n",
      "  \"benchmarks\": [\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"arc_easy\",\n",
      "      \"weight\": 1.5,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 25,\n",
      "        \"limit\": 100\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"humaneval\",\n",
      "      \"weight\": 2.0,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 0,\n",
      "        \"limit\": 50\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"mbpp\",\n",
      "      \"weight\": 2.0,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 0,\n",
      "        \"limit\": 50\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"provider_id\": \"lm_evaluation_harness\",\n",
      "      \"benchmark_id\": \"bbh\",\n",
      "      \"weight\": 1.5,\n",
      "      \"config\": {\n",
      "        \"num_fewshot\": 3,\n",
      "        \"limit\": 100\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created_at\": \"2025-11-09T23:48:17.846931Z\",\n",
      "  \"updated_at\": \"2025-11-09T23:48:17.846931Z\"\n",
      "}\n",
      "--------------------------------------------------\n",
      "📋 Collection: Coding & Reasoning Collection v1\n",
      "Description: A curated collection of coding and reasoning benchmarks using available lm-evaluation-harness tasks\n",
      "Provider: None\n",
      "\n",
      "🎯 Benchmarks (4):\n",
      "  - arc_easy (weight: 1.5, 21.4%)\n",
      "    Config: 25 shots, limit 100\n",
      "  - humaneval (weight: 2.0, 28.6%)\n",
      "    Config: 0 shots, limit 50\n",
      "  - mbpp (weight: 2.0, 28.6%)\n",
      "    Config: 0 shots, limit 50\n",
      "  - bbh (weight: 1.5, 21.4%)\n",
      "    Config: 3 shots, limit 100\n",
      "\n",
      "📊 Metadata:\n",
      "  Estimated duration: 30 minutes\n",
      "  Difficulty: intermediate_to_hard\n",
      "  Use case: coding_reasoning_assessment\n"
     ]
    }
   ],
   "source": [
    "# Get detailed information about our coding & reasoning collection\n",
    "collection_id = coding_reasoning_collection_id\n",
    "response = api_request(\"GET\", f\"/collections/{collection_id}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    collection = response.json()\n",
    "    print(f\"📋 Collection: {collection['name']}\")\n",
    "    print(f\"Description: {collection['description']}\")\n",
    "    print(f\"Provider: {collection['provider_id']}\")\n",
    "\n",
    "    print(f\"\\n🎯 Benchmarks ({len(collection['benchmarks'])}):\")\n",
    "    total_weight = sum(b.get('weight', 1.0) for b in collection['benchmarks'])\n",
    "\n",
    "    for benchmark in collection['benchmarks']:\n",
    "        weight = benchmark.get('weight', 1.0)\n",
    "        weight_pct = (weight / total_weight) * 100\n",
    "        print(f\"  - {benchmark['benchmark_id']} (weight: {weight}, {weight_pct:.1f}%)\")\n",
    "        if benchmark.get('config'):\n",
    "            config = benchmark['config']\n",
    "            print(f\"    Config: {config.get('num_fewshot', 0)} shots, limit {config.get('limit', 'unlimited')}\")\n",
    "\n",
    "    if collection.get('metadata'):\n",
    "        metadata = collection['metadata']\n",
    "        print(\"\\n📊 Metadata:\")\n",
    "        print(f\"  Estimated duration: {metadata.get('estimated_duration_minutes', 'unknown')} minutes\")\n",
    "        print(f\"  Difficulty: {metadata.get('difficulty', 'unknown')}\")\n",
    "        print(f\"  Use case: {metadata.get('use_case', 'unknown')}\")\n",
    "elif response.status_code == 404:\n",
    "    print(f\"❌ Collection '{collection_id}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection-Based Evaluations\n",
    "\n",
    "### Execute Evaluation Using a Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Creating collection-based evaluation...\n",
      "Collection ID: coding_reasoning_v1\n",
      "✨ Using native collection_id support - automatic expansion!\n",
      "{\n",
      "  \"request_id\": \"7884e84a-2443-4b26-b1b4-d5c7cb44d8ea\",\n",
      "  \"experiment_name\": \"Coding & Reasoning Collection Evaluation - coding_reasoning_v1\",\n",
      "  \"evaluations\": [\n",
      "    {\n",
      "      \"name\": \"TinyLlama Coding & Reasoning\",\n",
      "      \"description\": \"Evaluation using coding_reasoning_v1 collection with automatic expansion\",\n",
      "      \"model\": {\n",
      "        \"server\": \"vllm\",\n",
      "        \"name\": \"tinyllama\",\n",
      "        \"configuration\": {\n",
      "          \"temperature\": 0.0,\n",
      "          \"max_tokens\": 512,\n",
      "          \"top_p\": 0.95\n",
      "        }\n",
      "      },\n",
      "      \"collection_id\": \"coding_reasoning_v1\",\n",
      "      \"timeout_minutes\": 60,\n",
      "      \"retry_attempts\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"tags\": {\n",
      "    \"evaluation_type\": \"collection\",\n",
      "    \"collection_id\": \"coding_reasoning_v1\",\n",
      "    \"model_family\": \"llama\",\n",
      "    \"evaluation_scope\": \"coding_reasoning\"\n",
      "  }\n",
      "}\n",
      "POST http://localhost:8000/api/v1/evaluations\n",
      "Status: 202\n",
      "Response:\n",
      "{\n",
      "  \"request_id\": \"7884e84a-2443-4b26-b1b4-d5c7cb44d8ea\",\n",
      "  \"status\": \"pending\",\n",
      "  \"total_evaluations\": 0,\n",
      "  \"completed_evaluations\": 0,\n",
      "  \"failed_evaluations\": 0,\n",
      "  \"results\": [],\n",
      "  \"aggregated_metrics\": {},\n",
      "  \"experiment_url\": \"http://mlflow:5000/#/experiments/exp_47558d57\",\n",
      "  \"created_at\": \"2025-11-09T23:48:22.673948Z\",\n",
      "  \"updated_at\": \"2025-11-09T23:48:22.674455Z\",\n",
      "  \"estimated_completion\": null,\n",
      "  \"progress_percentage\": 0.0\n",
      "}\n",
      "--------------------------------------------------\n",
      "✅ Collection evaluation created successfully!\n",
      "Request ID: 7884e84a-2443-4b26-b1b4-d5c7cb44d8ea\n",
      "Status: pending\n",
      "Experiment URL: http://mlflow:5000/#/experiments/exp_47558d57\n",
      "\n",
      "✨ Native Collection Processing:\n",
      "  ✅ Collection ID: coding_reasoning_v1\n",
      "  ✅ Automatic backend expansion by eval-hub\n",
      "  ✅ Benchmark configs and weights preserved\n",
      "  🔄 Execution: Creating CR and running evaluation\n"
     ]
    }
   ],
   "source": [
    "# Create an evaluation request using our coding & reasoning collection\n",
    "# Now with native collection_id support in eval-hub!\n",
    "\n",
    "collection_evaluation = {\n",
    "    \"request_id\": str(uuid4()),\n",
    "    \"experiment_name\": f\"Coding & Reasoning Collection Evaluation - {coding_reasoning_collection_id}\",\n",
    "    \"evaluations\": [\n",
    "        {\n",
    "            \"name\": \"TinyLlama Coding & Reasoning\",\n",
    "            \"description\": f\"Evaluation using {coding_reasoning_collection_id} collection with automatic expansion\",\n",
    "            \"model\": {\n",
    "                \"server\": \"vllm\",  # Use the vLLM server from our Kubernetes setup\n",
    "                \"name\": \"tinyllama\",\n",
    "                \"configuration\": {\n",
    "                    \"temperature\": 0.0,  # Deterministic for benchmarking\n",
    "                    \"max_tokens\": 512,\n",
    "                    \"top_p\": 0.95\n",
    "                }\n",
    "            },\n",
    "            \"collection_id\": coding_reasoning_collection_id,  # ✨ Native collection support!\n",
    "            \"timeout_minutes\": 60,  # Allow more time for collection execution\n",
    "            \"retry_attempts\": 1\n",
    "        }\n",
    "    ],\n",
    "    \"tags\": {\n",
    "        \"evaluation_type\": \"collection\",\n",
    "        \"collection_id\": coding_reasoning_collection_id,\n",
    "        \"model_family\": \"llama\",\n",
    "        \"evaluation_scope\": \"coding_reasoning\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📝 Creating collection-based evaluation...\")\n",
    "print(f\"Collection ID: {coding_reasoning_collection_id}\")\n",
    "print(\"✨ Using native collection_id support - automatic expansion!\")\n",
    "\n",
    "print_json(collection_evaluation)\n",
    "\n",
    "response = api_request(\"POST\", \"/evaluations\", json=collection_evaluation)\n",
    "\n",
    "if response.status_code == 202:\n",
    "    collection_eval_response = response.json()\n",
    "    collection_request_id = collection_eval_response[\"request_id\"]\n",
    "    print(\"✅ Collection evaluation created successfully!\")\n",
    "    print(f\"Request ID: {collection_request_id}\")\n",
    "    print(f\"Status: {collection_eval_response['status']}\")\n",
    "    print(f\"Experiment URL: {collection_eval_response.get('experiment_url', 'N/A')}\")\n",
    "\n",
    "    # Automatic collection expansion process:\n",
    "    # 1. ✅ Eval-hub automatically looks up the collection by ID\n",
    "    # 2. ✅ Extracts all benchmarks from the collection\n",
    "    # 3. ✅ Groups benchmarks by provider\n",
    "    # 4. ✅ Creates appropriate backend configurations\n",
    "    # 5. 🔄 Will execute with proper weights and configurations\n",
    "    print(\"\\n✨ Native Collection Processing:\")\n",
    "    print(f\"  ✅ Collection ID: {coding_reasoning_collection_id}\")\n",
    "    print(\"  ✅ Automatic backend expansion by eval-hub\")\n",
    "    print(\"  ✅ Benchmark configs and weights preserved\")\n",
    "    print(\"  🔄 Execution: Creating CR and running evaluation\")\n",
    "else:\n",
    "    print(\"❌ Failed to create collection evaluation\")\n",
    "    print(f\"Error: {response.text}\")\n",
    "    collection_request_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Multiple Collections in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Creating 2 collection comparison evaluations...\n",
      "\n",
      "📝 Creating Coding & Reasoning collection evaluation...\n",
      "POST http://localhost:8000/api/v1/evaluations\n",
      "Status: 202\n",
      "Response:\n",
      "{\n",
      "  \"request_id\": \"0b93cd80-aaf3-48b4-aaa2-775925142a57\",\n",
      "  \"status\": \"pending\",\n",
      "  \"total_evaluations\": 0,\n",
      "  \"completed_evaluations\": 0,\n",
      "  \"failed_evaluations\": 0,\n",
      "  \"results\": [],\n",
      "  \"aggregated_metrics\": {},\n",
      "  \"experiment_url\": \"http://mlflow:5000/#/experiments/exp_2cec8da2\",\n",
      "  \"created_at\": \"2025-11-09T23:48:25.055453Z\",\n",
      "  \"updated_at\": \"2025-11-09T23:48:25.055884Z\",\n",
      "  \"estimated_completion\": null,\n",
      "  \"progress_percentage\": 0.0\n",
      "}\n",
      "--------------------------------------------------\n",
      "✅ Coding & Reasoning evaluation created: 0b93cd80-aaf3-48b4-aaa2-775925142a57\n",
      "\n",
      "📝 Creating Language Understanding collection evaluation...\n",
      "POST http://localhost:8000/api/v1/evaluations\n",
      "Status: 202\n",
      "Response:\n",
      "{\n",
      "  \"request_id\": \"e2f83bf3-fd65-442c-a260-a638929cc58c\",\n",
      "  \"status\": \"pending\",\n",
      "  \"total_evaluations\": 0,\n",
      "  \"completed_evaluations\": 0,\n",
      "  \"failed_evaluations\": 0,\n",
      "  \"results\": [],\n",
      "  \"aggregated_metrics\": {},\n",
      "  \"experiment_url\": \"http://mlflow:5000/#/experiments/exp_c4e2418f\",\n",
      "  \"created_at\": \"2025-11-09T23:48:25.425826Z\",\n",
      "  \"updated_at\": \"2025-11-09T23:48:25.426288Z\",\n",
      "  \"estimated_completion\": null,\n",
      "  \"progress_percentage\": 0.0\n",
      "}\n",
      "--------------------------------------------------\n",
      "✅ Language Understanding evaluation created: e2f83bf3-fd65-442c-a260-a638929cc58c\n",
      "\n",
      "📊 Created 2 collection comparisons\n",
      "  - Coding & Reasoning: 0b93cd80-aaf3-48b4-aaa2-775925142a57\n",
      "  - Language Understanding: e2f83bf3-fd65-442c-a260-a638929cc58c\n"
     ]
    }
   ],
   "source": [
    "# Create evaluations for both collections to compare different reasoning approaches\n",
    "collections_comparison = []\n",
    "\n",
    "for collection_id, collection_name in [\n",
    "    (coding_reasoning_collection_id, \"Coding & Reasoning\"),\n",
    "    (language_collection_id, \"Language Understanding\")\n",
    "]:\n",
    "    comparison_eval = {\n",
    "        \"request_id\": str(uuid4()),\n",
    "        \"experiment_name\": f\"{collection_name} Collection - Model Comparison\",\n",
    "        \"evaluations\": [\n",
    "            {\n",
    "                \"name\": f\"TinyLlama {collection_name} Evaluation\",\n",
    "                \"description\": f\"Comparative evaluation using {collection_id} collection\",\n",
    "                \"model\": {\n",
    "                    \"server\": \"vllm\",\n",
    "                    \"name\": \"tinyllama\",\n",
    "                    \"configuration\": {\n",
    "                        \"temperature\": 0.0,\n",
    "                        \"max_tokens\": 512\n",
    "                    }\n",
    "                },\n",
    "                \"collection_id\": collection_id,  # Just reference the collection!\n",
    "                \"timeout_minutes\": 90,\n",
    "                \"retry_attempts\": 1\n",
    "            }\n",
    "        ],\n",
    "        \"tags\": {\n",
    "            \"evaluation_type\": \"collection_comparison\",\n",
    "            \"collection_id\": collection_id,\n",
    "            \"comparison_group\": \"coding_vs_language\",\n",
    "            \"model\": \"tinyllama\"\n",
    "        }\n",
    "    }\n",
    "    collections_comparison.append(comparison_eval)\n",
    "\n",
    "print(f\"📦 Creating {len(collections_comparison)} collection comparison evaluations...\")\n",
    "\n",
    "comparison_request_ids = []\n",
    "for i, eval_request in enumerate(collections_comparison):\n",
    "    collection_name = [\"Coding & Reasoning\", \"Language Understanding\"][i]\n",
    "    print(f\"\\n📝 Creating {collection_name} collection evaluation...\")\n",
    "\n",
    "    response = api_request(\"POST\", \"/evaluations\", json=eval_request)\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        comparison_response = response.json()\n",
    "        comparison_request_ids.append(comparison_response[\"request_id\"])\n",
    "        print(f\"✅ {collection_name} evaluation created: {comparison_response['request_id']}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to create {collection_name} evaluation\")\n",
    "\n",
    "print(f\"\\n📊 Created {len(comparison_request_ids)} collection comparisons\")\n",
    "for i, req_id in enumerate(comparison_request_ids):\n",
    "    collection_type = [\"Coding & Reasoning\", \"Language Understanding\"][i]\n",
    "    print(f\"  - {collection_type}: {req_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection Results Management\n",
    "\n",
    "### Monitor Collection Evaluation Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function specifically for monitoring collection-based evaluations\n",
    "def monitor_collection_evaluation(request_id: str, collection_id: str):\n",
    "    \"\"\"Monitor a collection-based evaluation with collection-specific details.\"\"\"\n",
    "    print(f\"🔍 Monitoring collection evaluation: {collection_id}\")\n",
    "    print(f\"Request ID: {request_id}\")\n",
    "\n",
    "    response = api_request(\"GET\", f\"/evaluations/{request_id}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        status_data = response.json()\n",
    "        print(\"\\n📊 Collection Evaluation Status:\")\n",
    "        print(f\"Status: {status_data['status']}\")\n",
    "        print(f\"Progress: {status_data.get('progress_percentage', 0):.1f}%\")\n",
    "\n",
    "        # Collection-specific information\n",
    "        if status_data.get('collection_id'):\n",
    "            print(f\"Collection: {status_data['collection_id']}\")\n",
    "\n",
    "        # Show benchmark-level progress if available\n",
    "        if status_data.get('results'):\n",
    "            print(f\"\\n📋 Benchmark Progress ({len(status_data['results'])} completed):\")\n",
    "            for result in status_data['results']:\n",
    "                benchmark_name = result.get('benchmark_name', 'unknown')\n",
    "                result_status = result.get('status', 'unknown')\n",
    "                print(f\"  - {benchmark_name}: {result_status}\")\n",
    "\n",
    "                # Show key metrics if available\n",
    "                if result.get('metrics'):\n",
    "                    metrics = result['metrics']\n",
    "                    key_metrics = []\n",
    "                    for metric_name, metric_value in list(metrics.items())[:2]:  # Show first 2 metrics\n",
    "                        if isinstance(metric_value, (int, float)):\n",
    "                            key_metrics.append(f\"{metric_name}: {metric_value:.3f}\")\n",
    "                    if key_metrics:\n",
    "                        print(f\"    Metrics: {', '.join(key_metrics)}\")\n",
    "\n",
    "        return status_data\n",
    "    else:\n",
    "        print(f\"❌ Failed to get collection evaluation status: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Monitor our collection evaluation if it exists\n",
    "if 'collection_request_id' in locals() and collection_request_id:\n",
    "    monitor_collection_evaluation(collection_request_id, coding_reasoning_collection_id)\n",
    "else:\n",
    "    print(\"No active collection evaluation to monitor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Complete Collection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get comprehensive collection results\n",
    "def get_collection_results(request_id: str, format_for_analysis: bool = True):\n",
    "    \"\"\"\n",
    "    Retrieve and format results for a collection-based evaluation.\n",
    "\n",
    "    Args:\n",
    "        request_id: The evaluation request ID\n",
    "        format_for_analysis: Whether to format results for analysis\n",
    "    \"\"\"\n",
    "    print(\"📊 Retrieving collection evaluation results...\")\n",
    "\n",
    "    response = api_request(\"GET\", f\"/evaluations/{request_id}\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"❌ Failed to retrieve results: {response.text}\")\n",
    "        return None\n",
    "\n",
    "    eval_data = response.json()\n",
    "\n",
    "    if eval_data.get('status') != 'completed':\n",
    "        print(f\"⏳ Evaluation not completed. Status: {eval_data.get('status')}\")\n",
    "        return None\n",
    "\n",
    "    collection_id = eval_data.get('collection_id')\n",
    "    print(\"✅ Collection evaluation completed!\")\n",
    "    print(f\"Collection ID: {collection_id}\")\n",
    "    print(f\"Total benchmarks: {len(eval_data.get('results', []))}\")\n",
    "\n",
    "    # Aggregate collection-level metrics\n",
    "    results = eval_data.get('results', [])\n",
    "\n",
    "    if not results:\n",
    "        print(\"No results found\")\n",
    "        return eval_data\n",
    "\n",
    "    print(\"\\n📋 Collection Results Summary:\")\n",
    "\n",
    "    # Calculate weighted average scores based on collection benchmark weights\n",
    "    total_weighted_score = 0\n",
    "    total_weight = 0\n",
    "    benchmark_scores = {}\n",
    "\n",
    "    for result in results:\n",
    "        benchmark_name = result.get('benchmark_name', 'unknown')\n",
    "        benchmark_status = result.get('status', 'unknown')\n",
    "\n",
    "        print(f\"\\n  📊 {benchmark_name}: {benchmark_status}\")\n",
    "\n",
    "        if result.get('metrics'):\n",
    "            metrics = result['metrics']\n",
    "\n",
    "            # Extract primary accuracy metric (common across lm-eval-harness benchmarks)\n",
    "            primary_score = None\n",
    "            for metric_name in ['acc', 'acc_norm', 'exact_match', 'score']:\n",
    "                if metric_name in metrics:\n",
    "                    primary_score = metrics[metric_name]\n",
    "                    break\n",
    "\n",
    "            if primary_score is not None:\n",
    "                if isinstance(primary_score, dict) and 'value' in primary_score:\n",
    "                    score_value = primary_score['value']\n",
    "                else:\n",
    "                    score_value = primary_score\n",
    "\n",
    "                benchmark_scores[benchmark_name] = score_value\n",
    "                print(f\"    Primary score: {score_value:.3f}\")\n",
    "\n",
    "                # Get benchmark weight from collection (default 1.0)\n",
    "                weight = 1.0  # Default weight\n",
    "                # Note: In a real implementation, you'd look up the weight from the collection definition\n",
    "\n",
    "                total_weighted_score += score_value * weight\n",
    "                total_weight += weight\n",
    "\n",
    "            # Show additional metrics\n",
    "            other_metrics = []\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                if metric_name not in ['acc', 'acc_norm', 'exact_match', 'score']:\n",
    "                    if isinstance(metric_value, (int, float)):\n",
    "                        other_metrics.append(f\"{metric_name}: {metric_value:.3f}\")\n",
    "                    elif isinstance(metric_value, dict) and 'value' in metric_value:\n",
    "                        other_metrics.append(f\"{metric_name}: {metric_value['value']:.3f}\")\n",
    "\n",
    "            if other_metrics:\n",
    "                print(f\"    Other metrics: {', '.join(other_metrics[:3])}\")  # Show first 3\n",
    "\n",
    "    # Calculate collection-level aggregate score\n",
    "    if total_weight > 0:\n",
    "        collection_avg_score = total_weighted_score / total_weight\n",
    "        print(f\"\\n🎯 Collection Aggregate Score: {collection_avg_score:.3f}\")\n",
    "        print(f\"   (Weighted average across {len(benchmark_scores)} benchmarks)\")\n",
    "\n",
    "    if format_for_analysis:\n",
    "        # Format results for further analysis\n",
    "        analysis_format = {\n",
    "            \"collection_id\": collection_id,\n",
    "            \"evaluation_id\": request_id,\n",
    "            \"status\": eval_data['status'],\n",
    "            \"completed_at\": eval_data.get('updated_at'),\n",
    "            \"aggregate_score\": collection_avg_score if 'collection_avg_score' in locals() else None,\n",
    "            \"benchmark_scores\": benchmark_scores,\n",
    "            \"benchmark_count\": len(results),\n",
    "            \"successful_benchmarks\": len([r for r in results if r.get('status') == 'completed']),\n",
    "            \"failed_benchmarks\": len([r for r in results if r.get('status') == 'failed']),\n",
    "            \"raw_results\": results\n",
    "        }\n",
    "\n",
    "        print(\"\\n📋 Analysis Format Summary:\")\n",
    "        print(f\"  Successful: {analysis_format['successful_benchmarks']}/{analysis_format['benchmark_count']}\")\n",
    "        print(f\"  Success rate: {(analysis_format['successful_benchmarks']/analysis_format['benchmark_count']*100):.1f}%\")\n",
    "\n",
    "        return analysis_format\n",
    "\n",
    "    return eval_data\n",
    "\n",
    "# Example usage with a completed evaluation\n",
    "if 'collection_request_id' in locals() and collection_request_id:\n",
    "    print(f\"📊 Attempting to retrieve results for: {collection_request_id}\")\n",
    "    collection_results = get_collection_results(collection_request_id)\n",
    "else:\n",
    "    print(\"📝 No collection evaluation request ID available for result retrieval\")\n",
    "    print(\"📖 Example of what collection results would look like:\")\n",
    "\n",
    "    # Show example collection results structure\n",
    "    example_collection_results = {\n",
    "        \"collection_id\": \"academic_reasoning_v1\",\n",
    "        \"evaluation_id\": \"12345678-1234-1234-1234-123456789012\",\n",
    "        \"status\": \"completed\",\n",
    "        \"aggregate_score\": 0.742,\n",
    "        \"benchmark_scores\": {\n",
    "            \"arc_easy\": 0.753,\n",
    "            \"arc_challenge\": 0.462,\n",
    "            \"hellaswag\": 0.789,\n",
    "            \"mmlu\": 0.654\n",
    "        },\n",
    "        \"benchmark_count\": 4,\n",
    "        \"successful_benchmarks\": 4,\n",
    "        \"failed_benchmarks\": 0\n",
    "    }\n",
    "\n",
    "    print_json(example_collection_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Collection Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare results across multiple collection evaluations\n",
    "def compare_collection_results(request_ids: list, collection_names: list = None):\n",
    "    \"\"\"Compare results across multiple collection evaluations.\"\"\"\n",
    "\n",
    "    if collection_names is None:\n",
    "        collection_names = [f\"Collection {i+1}\" for i in range(len(request_ids))]\n",
    "\n",
    "    print(f\"📊 Comparing {len(request_ids)} collection evaluations...\")\n",
    "\n",
    "    comparison_data = []\n",
    "\n",
    "    for i, request_id in enumerate(request_ids):\n",
    "        collection_name = collection_names[i]\n",
    "        print(f\"\\n🔍 Retrieving results for {collection_name}...\")\n",
    "\n",
    "        results = get_collection_results(request_id, format_for_analysis=True)\n",
    "\n",
    "        if results:\n",
    "            comparison_data.append({\n",
    "                \"name\": collection_name,\n",
    "                \"request_id\": request_id,\n",
    "                \"collection_id\": results.get('collection_id'),\n",
    "                \"aggregate_score\": results.get('aggregate_score'),\n",
    "                \"benchmark_scores\": results.get('benchmark_scores', {}),\n",
    "                \"success_rate\": results.get('successful_benchmarks', 0) / max(results.get('benchmark_count', 1), 1),\n",
    "                \"benchmark_count\": results.get('benchmark_count', 0)\n",
    "            })\n",
    "\n",
    "    if not comparison_data:\n",
    "        print(\"❌ No valid results to compare\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n📊 Collection Performance Comparison:\")\n",
    "    print(f\"{'Collection':<25} {'Aggregate':<10} {'Success Rate':<12} {'Benchmarks':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for data in comparison_data:\n",
    "        aggregate = f\"{data['aggregate_score']:.3f}\" if data['aggregate_score'] else \"N/A\"\n",
    "        success_rate = f\"{data['success_rate']*100:.1f}%\" if data['success_rate'] else \"N/A\"\n",
    "        benchmarks = str(data['benchmark_count'])\n",
    "\n",
    "        print(f\"{data['name']:<25} {aggregate:<10} {success_rate:<12} {benchmarks:<10}\")\n",
    "\n",
    "    # Show benchmark-by-benchmark comparison if there are common benchmarks\n",
    "    all_benchmarks = set()\n",
    "    for data in comparison_data:\n",
    "        all_benchmarks.update(data['benchmark_scores'].keys())\n",
    "\n",
    "    if all_benchmarks:\n",
    "        print(\"\\n📋 Benchmark-by-Benchmark Comparison:\")\n",
    "\n",
    "        for benchmark in sorted(all_benchmarks):\n",
    "            print(f\"\\n  {benchmark}:\")\n",
    "            for data in comparison_data:\n",
    "                score = data['benchmark_scores'].get(benchmark)\n",
    "                score_str = f\"{score:.3f}\" if score is not None else \"N/A\"\n",
    "                print(f\"    {data['name']:<20}: {score_str}\")\n",
    "\n",
    "    return comparison_data\n",
    "\n",
    "# Example usage with comparison request IDs\n",
    "if 'comparison_request_ids' in locals() and comparison_request_ids:\n",
    "    collection_comparison = compare_collection_results(\n",
    "        comparison_request_ids,\n",
    "        [\"Coding & Reasoning\", \"Language Understanding\"]\n",
    "    )\n",
    "else:\n",
    "    print(\"📝 No comparison evaluations available\")\n",
    "    print(\"📖 This would compare performance across different collections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Collection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to export collection results for external analysis\n",
    "def export_collection_results(results_data: dict, filename: str = None):\n",
    "    \"\"\"Export collection results to JSON file for external analysis.\"\"\"\n",
    "\n",
    "    if filename is None:\n",
    "        collection_id = results_data.get('collection_id', 'unknown')\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"collection_results_{collection_id}_{timestamp}.json\"\n",
    "\n",
    "    # Prepare export format\n",
    "    export_data = {\n",
    "        \"export_metadata\": {\n",
    "            \"export_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S UTC\"),\n",
    "            \"eval_hub_version\": \"v1\",\n",
    "            \"format_version\": \"1.0\"\n",
    "        },\n",
    "        \"collection_evaluation\": results_data\n",
    "    }\n",
    "\n",
    "    # Write to file\n",
    "    import json\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"💾 Collection results exported to: {filename}\")\n",
    "    print(\"📊 Export contains:\")\n",
    "    print(f\"  - Collection ID: {results_data.get('collection_id', 'N/A')}\")\n",
    "    print(f\"  - Benchmarks: {results_data.get('benchmark_count', 0)}\")\n",
    "    print(f\"  - Aggregate score: {results_data.get('aggregate_score', 'N/A')}\")\n",
    "\n",
    "    return filename\n",
    "\n",
    "# Example export\n",
    "example_export_data = {\n",
    "    \"collection_id\": \"coding_reasoning_v1\",\n",
    "    \"aggregate_score\": 0.678,\n",
    "    \"benchmark_count\": 4,\n",
    "    \"benchmark_scores\": {\"arc_easy\": 0.753, \"humaneval\": 0.645, \"mbpp\": 0.672, \"bbh\": 0.642}\n",
    "}\n",
    "\n",
    "print(\"📝 Example collection results export:\")\n",
    "export_filename = export_collection_results(example_export_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Server Management\n",
    "\n",
    "### List All Model Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/servers\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    servers_data = response.json()\n",
    "    print(f\"Total servers: {servers_data['total_servers']}\")\n",
    "    print(f\"Runtime servers: {len(servers_data.get('runtime_servers', []))}\")\n",
    "\n",
    "    print(\"\\n📋 Model Servers:\")\n",
    "    for server in servers_data.get('servers', []):\n",
    "        print(f\"  - {server['server_id']}\")\n",
    "        print(f\"    Type: {server['server_type']}\")\n",
    "        print(f\"    Base URL: {server['base_url']}\")\n",
    "        print(f\"    Models: {server['model_count']}\")\n",
    "        print(f\"    Status: {server['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Only Active Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/servers\", params={\"include_inactive\": False})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    servers_data = response.json()\n",
    "    print(f\"Active servers: {servers_data['total_servers']}\")\n",
    "    for server in servers_data.get('servers', []):\n",
    "        print(f\"  - {server['server_id']} - {server['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Server by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get details for a specific model server\n",
    "server_id = \"vllm\"  # Replace with an actual server ID from your system\n",
    "response = api_request(\"GET\", f\"/servers/{server_id}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    server = response.json()\n",
    "    print(f\"Server ID: {server['server_id']}\")\n",
    "    print(f\"Type: {server['server_type']}\")\n",
    "    print(f\"Base URL: {server['base_url']}\")\n",
    "    print(f\"Status: {server['status']}\")\n",
    "\n",
    "    print(f\"\\n📦 Models on this server ({len(server['models'])}):\")\n",
    "    for model in server['models']:\n",
    "        print(f\"  - {model['model_name']}\")\n",
    "        print(f\"    Status: {model['status']}\")\n",
    "        if model.get('description'):\n",
    "            print(f\"    Description: {model['description']}\")\n",
    "\n",
    "    if server.get('tags'):\n",
    "        print(f\"\\nTags: {', '.join(server['tags'])}\")\n",
    "elif response.status_code == 404:\n",
    "    print(f\"❌ Server '{server_id}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model by Server and Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific model by getting the server and finding the model in its models list\n",
    "server_id = \"vllm\"\n",
    "model_name = \"vllm\"  # Replace with actual model name\n",
    "\n",
    "response = api_request(\"GET\", f\"/servers/{server_id}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    server = response.json()\n",
    "    model = None\n",
    "    for m in server['models']:\n",
    "        if m['model_name'] == model_name:\n",
    "            model = m\n",
    "            break\n",
    "\n",
    "    if model:\n",
    "        print(f\"✅ Found model: {model['model_name']}\")\n",
    "        print(f\"   Server: {server['server_id']}\")\n",
    "        print(f\"   Status: {model['status']}\")\n",
    "        if model.get('description'):\n",
    "            print(f\"   Description: {model['description']}\")\n",
    "        if model.get('capabilities'):\n",
    "            print(f\"   Capabilities: {model['capabilities']}\")\n",
    "    else:\n",
    "        print(f\"❌ Model '{model_name}' not found on server '{server_id}'\")\n",
    "else:\n",
    "    print(f\"❌ Server '{server_id}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a New Model Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a model server with models\n",
    "new_server = {\n",
    "    \"server_id\": \"groq-server\",\n",
    "    \"server_type\": \"openai-compatible\",\n",
    "    \"base_url\": \"https://api.groq.com/openai/v1\",\n",
    "    \"api_key_required\": True,\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"model_name\": \"llama-3.1-70b\",\n",
    "            \"description\": \"Meta's Llama 3.1 70B model\",\n",
    "            \"status\": \"active\",\n",
    "            \"tags\": [\"groq\", \"llama\", \"70b\"]\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"llama-3.1-8b\",\n",
    "            \"description\": \"Meta's Llama 3.1 8B model\",\n",
    "            \"status\": \"active\",\n",
    "            \"tags\": [\"groq\", \"llama\", \"8b\"]\n",
    "        }\n",
    "    ],\n",
    "    \"server_config\": {\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 2048,\n",
    "        \"timeout\": 60,\n",
    "        \"retry_attempts\": 3\n",
    "    },\n",
    "    \"status\": \"active\",\n",
    "    \"tags\": [\"groq\", \"openai-compatible\", \"fast\"]\n",
    "}\n",
    "\n",
    "print(\"📝 Registering new model server...\")\n",
    "print_json(new_server)\n",
    "\n",
    "response = api_request(\"POST\", \"/servers\", json=new_server)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    registered_server = response.json()\n",
    "    print(\"✅ Model server registered successfully!\")\n",
    "    print(f\"Server ID: {registered_server['server_id']}\")\n",
    "    print(f\"Models: {len(registered_server['models'])}\")\n",
    "    print(f\"Created at: {registered_server.get('created_at', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"❌ Failed to register server: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a vLLM Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a vLLM server\n",
    "vllm_server = {\n",
    "    \"server_id\": \"local-vllm\",\n",
    "    \"server_type\": \"vllm\",\n",
    "    \"base_url\": \"http://localhost:8000\",\n",
    "    \"api_key_required\": False,\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"model_name\": \"llama-2-7b\",\n",
    "            \"description\": \"Llama 2 7B running on local vLLM server\",\n",
    "            \"status\": \"active\",\n",
    "            \"tags\": [\"vllm\", \"local\", \"llama-2\"]\n",
    "        }\n",
    "    ],\n",
    "    \"status\": \"active\",\n",
    "    \"tags\": [\"vllm\", \"local\"]\n",
    "}\n",
    "\n",
    "print(\"📝 Registering vLLM server...\")\n",
    "response = api_request(\"POST\", \"/servers\", json=vllm_server)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    print(f\"✅ vLLM server registered: {response.json()['server_id']}\")\n",
    "else:\n",
    "    print(\"⚠️ Note: This may fail if the server ID already exists\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Evaluation Examples\n",
    "\n",
    "### Single Benchmark Evaluation from Builtin Provider (Simplified API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run a single benchmark using the simplified API (Llama Stack compatible)\n",
    "provider_id = \"lm_evaluation_harness\"\n",
    "benchmark_id = \"arc_easy\"\n",
    "\n",
    "single_benchmark_request = {\n",
    "    \"model\": {\n",
    "        \"server\": \"vllm\",\n",
    "        \"name\": \"tinyllama\"\n",
    "    },\n",
    "    \"model_configuration\": {\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 512\n",
    "    },\n",
    "    \"timeout_minutes\": 30,\n",
    "    \"retry_attempts\": 1,\n",
    "    \"limit\": 100,  # Limit to 100 samples for faster execution\n",
    "    \"num_fewshot\": 0,\n",
    "    \"experiment_name\": \"Single Benchmark - ARC Easy\",\n",
    "    \"tags\": {\n",
    "        \"example_type\": \"single_benchmark\",\n",
    "        \"provider\": \"lm_evaluation_harness\",\n",
    "        \"benchmark\": \"arc_easy\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📝 Creating single benchmark evaluation request...\")\n",
    "print(f\"Provider ID: {provider_id}\")\n",
    "print(f\"Benchmark ID: {benchmark_id}\")\n",
    "print_json(single_benchmark_request)\n",
    "\n",
    "response = api_request(\"POST\", f\"/evaluations/benchmarks/{provider_id}/{benchmark_id}\", json=single_benchmark_request)\n",
    "\n",
    "if response.status_code == 202:\n",
    "    evaluation_response = response.json()\n",
    "    request_id = evaluation_response[\"request_id\"]\n",
    "    print(\"✅ Single benchmark evaluation created successfully!\")\n",
    "    print(f\"Request ID: {request_id}\")\n",
    "    print(f\"Status: {evaluation_response['status']}\")\n",
    "    print(f\"Experiment URL: {evaluation_response.get('experiment_url', 'N/A')}\")\n",
    "else:\n",
    "    print(\"❌ Failed to create evaluation\")\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Evaluation with Risk Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple evaluation request using risk category\n",
    "evaluation_request = {\n",
    "    \"request_id\": str(uuid4()),\n",
    "    \"experiment_name\": \"Simple Risk-Based Evaluation\",\n",
    "    \"evaluations\": [\n",
    "        {\n",
    "            \"name\": \"GPT-4 Mini Low Risk Evaluation\",\n",
    "            \"description\": \"Basic evaluation using low risk benchmarks\",\n",
    "            \"model\": {\n",
    "                \"server\": \"default\",\n",
    "                \"name\": \"default\"\n",
    "            },\n",
    "            \"model_configuration\": {\n",
    "                \"temperature\": 0.0,\n",
    "                \"max_tokens\": 512\n",
    "            },\n",
    "            \"risk_category\": \"low\",\n",
    "            \"timeout_minutes\": 30,\n",
    "            \"retry_attempts\": 1\n",
    "        }\n",
    "    ],\n",
    "    \"tags\": {\n",
    "        \"example_type\": \"risk_category\",\n",
    "        \"complexity\": \"simple\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📝 Creating evaluation request...\")\n",
    "print_json(evaluation_request)\n",
    "\n",
    "response = api_request(\"POST\", \"/evaluations\", json=evaluation_request)\n",
    "\n",
    "if response.status_code == 202:\n",
    "    evaluation_response = response.json()\n",
    "    request_id = evaluation_response[\"request_id\"]\n",
    "    print(\"✅ Evaluation created successfully!\")\n",
    "    print(f\"Request ID: {request_id}\")\n",
    "    print(f\"Status: {evaluation_response['status']}\")\n",
    "    print(f\"Experiment URL: {evaluation_response.get('experiment_url', 'N/A')}\")\n",
    "else:\n",
    "    print(\"❌ Failed to create evaluation\")\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Explicit Backend Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation with explicit backend configuration\n",
    "explicit_evaluation = {\n",
    "    \"request_id\": str(uuid4()),\n",
    "    \"experiment_name\": \"Explicit Backend Configuration\",\n",
    "    \"evaluations\": [\n",
    "        {\n",
    "            \"name\": \"LM-Eval Harness Evaluation\",\n",
    "            \"description\": \"Evaluation with explicit lm-evaluation-harness configuration\",\n",
    "            \"model\": {\n",
    "                \"server\": \"default\",\n",
    "                \"name\": \"default\"\n",
    "            },\n",
    "            \"model_configuration\": {\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 256,\n",
    "                \"top_p\": 0.95\n",
    "            },\n",
    "            \"backends\": [\n",
    "                {\n",
    "                    \"name\": \"lm-eval-backend\",\n",
    "                    \"type\": \"lm-evaluation-harness\",\n",
    "                    \"config\": {\n",
    "                        \"batch_size\": 1,\n",
    "                        \"device\": \"cpu\"\n",
    "                    },\n",
    "                    \"benchmarks\": [\n",
    "                        {\n",
    "                            \"name\": \"arc_easy\",\n",
    "                            \"tasks\": [\"arc_easy\"],\n",
    "                            \"config\": {\n",
    "                                \"num_fewshot\": 5,\n",
    "                                \"limit\": 50\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"hellaswag\",\n",
    "                            \"tasks\": [\"hellaswag\"],\n",
    "                            \"config\": {\n",
    "                                \"num_fewshot\": 10,\n",
    "                                \"limit\": 100\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"timeout_minutes\": 45,\n",
    "            \"retry_attempts\": 2\n",
    "        }\n",
    "    ],\n",
    "    \"tags\": {\n",
    "        \"example_type\": \"explicit_backend\",\n",
    "        \"complexity\": \"intermediate\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📝 Creating evaluation with explicit backend...\")\n",
    "response = api_request(\"POST\", \"/evaluations\", json=explicit_evaluation)\n",
    "\n",
    "if response.status_code == 202:\n",
    "    explicit_response = response.json()\n",
    "    explicit_request_id = explicit_response[\"request_id\"]\n",
    "    print(\"✅ Explicit evaluation created!\")\n",
    "    print(f\"Request ID: {explicit_request_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeMo Evaluator Integration\n",
    "\n",
    "### Single NeMo Evaluator Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with single NeMo Evaluator container\n",
    "nemo_single_evaluation = {\n",
    "    \"request_id\": str(uuid4()),\n",
    "    \"experiment_name\": \"NeMo Evaluator Single Container\",\n",
    "    \"evaluations\": [\n",
    "        {\n",
    "            \"name\": \"GPT-4 via NeMo Evaluator\",\n",
    "            \"description\": \"Remote evaluation using NeMo Evaluator container\",\n",
    "            \"model\": {\n",
    "                \"server\": \"default\",\n",
    "                \"name\": \"default\"\n",
    "            },\n",
    "            \"model_configuration\": {\n",
    "                \"temperature\": 0.0,\n",
    "                \"max_tokens\": 512,\n",
    "                \"top_p\": 0.95\n",
    "            },\n",
    "            \"backends\": [\n",
    "                {\n",
    "                    \"name\": \"remote-nemo-evaluator\",\n",
    "                    \"type\": \"nemo-evaluator\",\n",
    "                    \"config\": {\n",
    "                        \"endpoint\": \"localhost\",\n",
    "                        \"port\": 3825,\n",
    "                        \"model_endpoint\": \"https://api.openai.com/v1/chat/completions\",\n",
    "                        \"endpoint_type\": \"chat\",\n",
    "                        \"api_key_env\": \"OPENAI_API_KEY\",\n",
    "                        \"timeout_seconds\": 1800,\n",
    "                        \"max_retries\": 2,\n",
    "                        \"verify_ssl\": False,\n",
    "                        \"framework_name\": \"eval-hub-example\",\n",
    "                        \"parallelism\": 1,\n",
    "                        \"limit_samples\": 25,\n",
    "                        \"temperature\": 0.0,\n",
    "                        \"top_p\": 0.95\n",
    "                    },\n",
    "                    \"benchmarks\": [\n",
    "                        {\n",
    "                            \"name\": \"mmlu_pro_sample\",\n",
    "                            \"tasks\": [\"mmlu_pro\"],\n",
    "                            \"config\": {\n",
    "                                \"limit\": 25,\n",
    "                                \"num_fewshot\": 5\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"timeout_minutes\": 60,\n",
    "            \"retry_attempts\": 1\n",
    "        }\n",
    "    ],\n",
    "    \"tags\": {\n",
    "        \"example_type\": \"nemo_evaluator_single\",\n",
    "        \"complexity\": \"advanced\",\n",
    "        \"backend\": \"remote_container\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📝 Creating NeMo Evaluator evaluation...\")\n",
    "print(\"Note: This requires a running NeMo Evaluator container on localhost:3825\")\n",
    "\n",
    "response = api_request(\"POST\", \"/evaluations\", json=nemo_single_evaluation)\n",
    "\n",
    "if response.status_code == 202:\n",
    "    nemo_response = response.json()\n",
    "    nemo_request_id = nemo_response[\"request_id\"]\n",
    "    print(\"✅ NeMo evaluation created!\")\n",
    "    print(f\"Request ID: {nemo_request_id}\")\n",
    "else:\n",
    "    print(\"⚠️ NeMo evaluation failed (container may not be running)\")\n",
    "    print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Container NeMo Evaluator Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with multiple specialized NeMo Evaluator containers\n",
    "nemo_multi_evaluation = {\n",
    "    \"request_id\": str(uuid4()),\n",
    "    \"experiment_name\": \"Multi-Container NeMo Evaluation\",\n",
    "    \"evaluations\": [\n",
    "        {\n",
    "            \"name\": \"Distributed LLaMA Evaluation\",\n",
    "            \"description\": \"Multi-container evaluation across specialized endpoints\",\n",
    "            \"model\": {\n",
    "                \"server\": \"default\",\n",
    "                \"name\": \"default\"\n",
    "            },\n",
    "            \"model_configuration\": {\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 512,\n",
    "                \"top_p\": 0.95\n",
    "            },\n",
    "            \"backends\": [\n",
    "                {\n",
    "                    \"name\": \"academic-evaluator\",\n",
    "                    \"type\": \"nemo-evaluator\",\n",
    "                    \"config\": {\n",
    "                        \"endpoint\": \"academic-eval.example.com\",\n",
    "                        \"port\": 3825,\n",
    "                        \"model_endpoint\": \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "                        \"endpoint_type\": \"chat\",\n",
    "                        \"api_key_env\": \"GROQ_API_KEY\",\n",
    "                        \"timeout_seconds\": 3600,\n",
    "                        \"framework_name\": \"eval-hub-academic\",\n",
    "                        \"parallelism\": 2\n",
    "                    },\n",
    "                    \"benchmarks\": [\n",
    "                        {\n",
    "                            \"name\": \"mmlu_pro\",\n",
    "                            \"tasks\": [\"mmlu_pro\"],\n",
    "                            \"config\": {\"limit\": 100, \"num_fewshot\": 5}\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"arc_challenge\",\n",
    "                            \"tasks\": [\"arc_challenge\"],\n",
    "                            \"config\": {\"limit\": 200, \"num_fewshot\": 25}\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"math-evaluator\",\n",
    "                    \"type\": \"nemo-evaluator\",\n",
    "                    \"config\": {\n",
    "                        \"endpoint\": \"math-eval.example.com\",\n",
    "                        \"port\": 3825,\n",
    "                        \"model_endpoint\": \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "                        \"endpoint_type\": \"chat\",\n",
    "                        \"api_key_env\": \"GROQ_API_KEY\",\n",
    "                        \"temperature\": 0.0,\n",
    "                        \"parallelism\": 1,\n",
    "                        \"framework_name\": \"eval-hub-math\"\n",
    "                    },\n",
    "                    \"benchmarks\": [\n",
    "                        {\n",
    "                            \"name\": \"gsm8k\",\n",
    "                            \"tasks\": [\"gsm8k\"],\n",
    "                            \"config\": {\"limit\": 100, \"num_fewshot\": 8}\n",
    "                        },\n",
    "                        {\n",
    "                            \"name\": \"math\",\n",
    "                            \"tasks\": [\"hendrycks_math\"],\n",
    "                            \"config\": {\"limit\": 50, \"num_fewshot\": 4}\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"timeout_minutes\": 120,\n",
    "            \"retry_attempts\": 2\n",
    "        }\n",
    "    ],\n",
    "    \"tags\": {\n",
    "        \"example_type\": \"nemo_evaluator_multi\",\n",
    "        \"complexity\": \"expert\",\n",
    "        \"backend\": \"distributed_containers\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📝 Creating multi-container NeMo evaluation...\")\n",
    "print(\"Note: This is a hypothetical example with multiple remote containers\")\n",
    "print_json(nemo_multi_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Status Monitoring\n",
    "\n",
    "### Check Evaluation Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check evaluation status\n",
    "def check_evaluation_status(request_id: str):\n",
    "    response = api_request(\"GET\", f\"/evaluations/{request_id}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        status_data = response.json()\n",
    "        print(f\"📊 Evaluation Status for {request_id}\")\n",
    "        print(f\"Status: {status_data['status']}\")\n",
    "        print(f\"Progress: {status_data.get('progress_percentage', 0):.1f}%\")\n",
    "        print(f\"Total evaluations: {status_data.get('total_evaluations', 0)}\")\n",
    "        print(f\"Completed: {status_data.get('completed_evaluations', 0)}\")\n",
    "        print(f\"Failed: {status_data.get('failed_evaluations', 0)}\")\n",
    "\n",
    "        if status_data.get('results'):\n",
    "            print(f\"Results available: {len(status_data['results'])}\")\n",
    "\n",
    "        return status_data\n",
    "    else:\n",
    "        print(f\"❌ Failed to get status: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Check status of previously created evaluations (if they exist)\n",
    "try:\n",
    "    if 'request_id' in locals():\n",
    "        check_evaluation_status(request_id)\n",
    "except NameError:\n",
    "    print(\"No evaluation request_id available to check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Evaluation Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to monitor evaluation until completion\n",
    "def monitor_evaluation(request_id: str, max_wait_time: int = 300):\n",
    "    \"\"\"Monitor an evaluation until completion or timeout.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    while time.time() - start_time < max_wait_time:\n",
    "        status_data = check_evaluation_status(request_id)\n",
    "\n",
    "        if not status_data:\n",
    "            break\n",
    "\n",
    "        status = status_data['status']\n",
    "\n",
    "        if status in ['completed', 'failed', 'cancelled']:\n",
    "            print(f\"🏁 Evaluation {status}!\")\n",
    "\n",
    "            if status == 'completed' and status_data.get('results'):\n",
    "                print(\"\\n📊 Results Summary:\")\n",
    "                for result in status_data['results'][:3]:  # Show first 3 results\n",
    "                    print(f\"  - {result['benchmark_name']}: {result['status']}\")\n",
    "                    if result.get('metrics'):\n",
    "                        for metric, value in list(result['metrics'].items())[:2]:\n",
    "                            print(f\"    {metric}: {value}\")\n",
    "\n",
    "            return status_data\n",
    "\n",
    "        print(f\"⏳ Still {status}, waiting...\")\n",
    "        time.sleep(10)\n",
    "\n",
    "    print(f\"⏰ Monitoring timed out after {max_wait_time} seconds\")\n",
    "    return None\n",
    "\n",
    "# Example usage (uncomment if you have a running evaluation)\n",
    "# monitor_evaluation(request_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List All Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/evaluations\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    evaluations = response.json()\n",
    "    print(f\"📋 Active evaluations: {len(evaluations)}\")\n",
    "\n",
    "    for eval_resp in evaluations:\n",
    "        print(f\"\\n🔍 {eval_resp['request_id']}\")\n",
    "        print(f\"   Status: {eval_resp['status']}\")\n",
    "        print(f\"   Progress: {eval_resp.get('progress_percentage', 0):.1f}%\")\n",
    "        print(f\"   Created: {eval_resp['created_at']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = api_request(\"GET\", \"/metrics/system\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    metrics = response.json()\n",
    "    print(\"📊 System Metrics:\")\n",
    "    print(f\"  Active evaluations: {metrics['active_evaluations']}\")\n",
    "    print(f\"  Running tasks: {metrics['running_tasks']}\")\n",
    "    print(f\"  Total requests: {metrics['total_requests']}\")\n",
    "\n",
    "    if metrics.get('status_breakdown'):\n",
    "        print(\"\\n  Status breakdown:\")\n",
    "        for status, count in metrics['status_breakdown'].items():\n",
    "            print(f\"    {status}: {count}\")\n",
    "\n",
    "    if metrics.get('memory_usage'):\n",
    "        print(\"\\n  Memory usage:\")\n",
    "        print(f\"    Active evaluations: {metrics['memory_usage']['active_evaluations_mb']:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Management\n",
    "\n",
    "### Cancel an Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cancel an evaluation\n",
    "def cancel_evaluation(request_id: str):\n",
    "    response = api_request(\"DELETE\", f\"/evaluations/{request_id}\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"✅ {result['message']}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"❌ Failed to cancel: {response.text}\")\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment if you want to cancel an evaluation)\n",
    "# cancel_evaluation(request_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling Examples\n",
    "\n",
    "### Invalid Request Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of invalid request to demonstrate error handling\n",
    "invalid_request = {\n",
    "    \"request_id\": \"invalid-uuid-format\",\n",
    "    \"evaluations\": [\n",
    "        {\n",
    "            \"name\": \"\",  # Invalid: empty name\n",
    "            \"model\": {\n",
    "                \"server\": \"\",  # Invalid: empty server\n",
    "                \"name\": \"\"  # Invalid: empty model name\n",
    "            },\n",
    "            \"backends\": []  # Invalid: no backends\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"📝 Testing error handling with invalid request...\")\n",
    "response = api_request(\"POST\", \"/evaluations\", json=invalid_request)\n",
    "\n",
    "if response.status_code >= 400:\n",
    "    print(\"✅ Error handling working correctly\")\n",
    "    error_data = response.json()\n",
    "    print(f\"Error type: {response.status_code}\")\n",
    "    print(f\"Error message: {error_data.get('detail', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-existent Resource Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accessing non-existent evaluation\n",
    "fake_request_id = str(uuid4())\n",
    "print(f\"🔍 Testing access to non-existent evaluation: {fake_request_id}\")\n",
    "\n",
    "response = api_request(\"GET\", f\"/evaluations/{fake_request_id}\")\n",
    "\n",
    "if response.status_code == 404:\n",
    "    print(\"✅ 404 handling working correctly\")\n",
    "    error_data = response.json()\n",
    "    print(f\"Error: {error_data['detail']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Examples\n",
    "\n",
    "### Batch Evaluation Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple evaluations for comparison\n",
    "batch_requests = []\n",
    "\n",
    "models_to_compare = [\"gpt-4o-mini\", \"gpt-3.5-turbo\"]\n",
    "risk_levels = [\"low\", \"medium\"]\n",
    "\n",
    "for model in models_to_compare:\n",
    "    for risk in risk_levels:\n",
    "        batch_request = {\n",
    "            \"request_id\": str(uuid4()),\n",
    "            \"experiment_name\": f\"Batch Comparison - {model} - {risk} risk\",\n",
    "            \"evaluations\": [\n",
    "                {\n",
    "                    \"name\": f\"{model} {risk} risk evaluation\",\n",
    "                    \"model\": {\n",
    "                        \"server\": \"default\",\n",
    "                        \"name\": \"default\"\n",
    "                    },\n",
    "                    \"model_configuration\": {\n",
    "                        \"temperature\": 0.0,\n",
    "                        \"max_tokens\": 256\n",
    "                    },\n",
    "                    \"risk_category\": risk,\n",
    "                    \"timeout_minutes\": 30\n",
    "                }\n",
    "            ],\n",
    "            \"tags\": {\n",
    "                \"batch_id\": \"model_comparison_001\",\n",
    "                \"model\": model,\n",
    "                \"risk_level\": risk\n",
    "            }\n",
    "        }\n",
    "        batch_requests.append(batch_request)\n",
    "\n",
    "print(f\"📦 Creating {len(batch_requests)} batch evaluations...\")\n",
    "\n",
    "batch_results = []\n",
    "for i, request in enumerate(batch_requests):\n",
    "    print(f\"\\n📝 Creating batch request {i+1}/{len(batch_requests)}\")\n",
    "    response = api_request(\"POST\", \"/evaluations\", json=request)\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        batch_results.append(response.json())\n",
    "        print(f\"✅ Batch {i+1} created: {response.json()['request_id']}\")\n",
    "    else:\n",
    "        print(f\"❌ Batch {i+1} failed\")\n",
    "\n",
    "print(f\"\\n📊 Successfully created {len(batch_results)} batch evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various configuration combinations\n",
    "test_configs = [\n",
    "    {\n",
    "        \"name\": \"High timeout test\",\n",
    "        \"config\": {\"timeout_minutes\": 120, \"retry_attempts\": 5},\n",
    "        \"expected\": \"success\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Zero timeout test\",\n",
    "        \"config\": {\"timeout_minutes\": 0, \"retry_attempts\": 1},\n",
    "        \"expected\": \"validation_error\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Negative retry test\",\n",
    "        \"config\": {\"timeout_minutes\": 30, \"retry_attempts\": -1},\n",
    "        \"expected\": \"validation_error\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for test in test_configs:\n",
    "    print(f\"\\n🧪 Testing: {test['name']}\")\n",
    "\n",
    "    test_request = {\n",
    "        \"request_id\": str(uuid4()),\n",
    "        \"experiment_name\": test['name'],\n",
    "        \"evaluations\": [\n",
    "            {\n",
    "                \"name\": \"Config test\",\n",
    "                \"model\": {\n",
    "                    \"server\": \"default\",\n",
    "                    \"name\": \"default\"\n",
    "                },\n",
    "                \"risk_category\": \"low\",\n",
    "                **test['config']\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = api_request(\"POST\", \"/evaluations\", json=test_request)\n",
    "\n",
    "    if test['expected'] == \"success\" and response.status_code == 202:\n",
    "        print(\"✅ Test passed\")\n",
    "    elif test['expected'] == \"validation_error\" and response.status_code >= 400:\n",
    "        print(\"✅ Validation correctly rejected invalid config\")\n",
    "    else:\n",
    "        print(f\"❌ Unexpected result: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and Formatting Results\n",
    "\n",
    "### Get Evaluation Results in NeMo Evaluator Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format eval-hub results to NeMo Evaluator compatible format\n",
    "def format_to_nemo_evaluator(eval_hub_result):\n",
    "    \"\"\"\n",
    "    Convert eval-hub result format to NeMo Evaluator EvaluationResult format.\n",
    "\n",
    "    Expected NeMo format:\n",
    "    {\n",
    "        \"tasks\": {\n",
    "            \"task_name\": {\n",
    "                \"metrics\": {\n",
    "                    \"metric_name\": {\n",
    "                        \"scores\": {\n",
    "                            \"score_name\": {\n",
    "                                \"value\": float,\n",
    "                                \"stats\": {\n",
    "                                    \"count\": int,\n",
    "                                    \"sum\": float,\n",
    "                                    \"mean\": float,\n",
    "                                    \"stderr\": float,\n",
    "                                    \"min\": float,\n",
    "                                    \"max\": float,\n",
    "                                    \"variance\": float,\n",
    "                                    \"stddev\": float\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"groups\": { ... }  # Same structure as tasks\n",
    "    }\n",
    "    \"\"\"\n",
    "    nemo_result = {\n",
    "        \"tasks\": {},\n",
    "        \"groups\": {}\n",
    "    }\n",
    "\n",
    "    # Extract benchmark results from eval-hub format\n",
    "    if 'results' in eval_hub_result:\n",
    "        for result in eval_hub_result['results']:\n",
    "            benchmark_name = result.get('benchmark_name', 'unknown_benchmark')\n",
    "            metrics = result.get('metrics', {})\n",
    "\n",
    "            # Convert metrics to NeMo format\n",
    "            nemo_metrics = {}\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                if isinstance(metric_value, (int, float)):\n",
    "                    # Simple scalar metric\n",
    "                    nemo_metrics[metric_name] = {\n",
    "                        \"scores\": {\n",
    "                            metric_name: {\n",
    "                                \"value\": float(metric_value),\n",
    "                                \"stats\": {\n",
    "                                    \"count\": 1,\n",
    "                                    \"sum\": float(metric_value),\n",
    "                                    \"mean\": float(metric_value),\n",
    "                                    \"stderr\": 0.0,\n",
    "                                    \"min\": float(metric_value),\n",
    "                                    \"max\": float(metric_value),\n",
    "                                    \"variance\": 0.0,\n",
    "                                    \"stddev\": 0.0\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                elif isinstance(metric_value, dict) and 'value' in metric_value:\n",
    "                    # Structured metric with stats\n",
    "                    stats = metric_value.get('stats', {})\n",
    "                    nemo_metrics[metric_name] = {\n",
    "                        \"scores\": {\n",
    "                            metric_name: {\n",
    "                                \"value\": float(metric_value['value']),\n",
    "                                \"stats\": {\n",
    "                                    \"count\": stats.get('count', 1),\n",
    "                                    \"sum\": stats.get('sum', metric_value['value']),\n",
    "                                    \"mean\": stats.get('mean', metric_value['value']),\n",
    "                                    \"stderr\": stats.get('stderr', 0.0),\n",
    "                                    \"min\": stats.get('min', metric_value['value']),\n",
    "                                    \"max\": stats.get('max', metric_value['value']),\n",
    "                                    \"variance\": stats.get('variance', 0.0),\n",
    "                                    \"stddev\": stats.get('stddev', 0.0)\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "            # Add to tasks\n",
    "            nemo_result[\"tasks\"][benchmark_name] = {\"metrics\": nemo_metrics}\n",
    "\n",
    "            # Add to groups (using provider as group name)\n",
    "            provider_id = result.get('provider_id', 'unknown_provider')\n",
    "            if provider_id not in nemo_result[\"groups\"]:\n",
    "                nemo_result[\"groups\"][provider_id] = {\"metrics\": {}}\n",
    "\n",
    "            # Aggregate metrics at group level\n",
    "            for metric_name, metric_data in nemo_metrics.items():\n",
    "                if metric_name not in nemo_result[\"groups\"][provider_id][\"metrics\"]:\n",
    "                    nemo_result[\"groups\"][provider_id][\"metrics\"][metric_name] = metric_data\n",
    "\n",
    "    return nemo_result\n",
    "\n",
    "# Example: Get results for a completed evaluation\n",
    "def get_evaluation_results_nemo_format(request_id: str):\n",
    "    \"\"\"Get evaluation results and format them for NeMo Evaluator compatibility.\"\"\"\n",
    "    print(f\"🔍 Retrieving results for evaluation: {request_id}\")\n",
    "\n",
    "    response = api_request(\"GET\", f\"/evaluations/{request_id}\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"❌ Failed to get evaluation results: {response.text}\")\n",
    "        return None\n",
    "\n",
    "    eval_data = response.json()\n",
    "\n",
    "    # Check if evaluation is completed\n",
    "    if eval_data.get('status') != 'completed':\n",
    "        print(f\"⏳ Evaluation not completed yet. Status: {eval_data.get('status')}\")\n",
    "        print(f\"📊 Progress: {eval_data.get('progress_percentage', 0):.1f}%\")\n",
    "        return None\n",
    "\n",
    "    print(\"✅ Evaluation completed successfully!\")\n",
    "    print(f\"📊 Total evaluations: {eval_data.get('total_evaluations', 0)}\")\n",
    "    print(f\"✅ Completed: {eval_data.get('completed_evaluations', 0)}\")\n",
    "    print(f\"❌ Failed: {eval_data.get('failed_evaluations', 0)}\")\n",
    "\n",
    "    # Format to NeMo Evaluator structure\n",
    "    nemo_formatted = format_to_nemo_evaluator(eval_data)\n",
    "\n",
    "    print(\"\\n🎯 Results formatted for NeMo Evaluator:\")\n",
    "    print_json(nemo_formatted)\n",
    "\n",
    "    return nemo_formatted\n",
    "\n",
    "# Example usage with a completed evaluation\n",
    "# Replace with an actual request_id from a completed evaluation\n",
    "example_request_id = \"00000000-0000-0000-0000-000000000000\"  # Placeholder\n",
    "\n",
    "print(\"📝 Example: Retrieving evaluation results...\")\n",
    "print(f\"Note: Replace '{example_request_id}' with actual request ID from completed evaluation\")\n",
    "\n",
    "# Simulated example of what the formatted result would look like\n",
    "example_nemo_result = {\n",
    "    \"tasks\": {\n",
    "        \"arc_easy\": {\n",
    "            \"metrics\": {\n",
    "                \"acc\": {\n",
    "                    \"scores\": {\n",
    "                        \"acc\": {\n",
    "                            \"value\": 0.7534,\n",
    "                            \"stats\": {\n",
    "                                \"count\": 2376,\n",
    "                                \"sum\": 1790.0,\n",
    "                                \"mean\": 0.7534,\n",
    "                                \"stderr\": 0.0088,\n",
    "                                \"min\": 0.0,\n",
    "                                \"max\": 1.0,\n",
    "                                \"variance\": 0.1856,\n",
    "                                \"stddev\": 0.4307\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"acc_norm\": {\n",
    "                    \"scores\": {\n",
    "                        \"acc_norm\": {\n",
    "                            \"value\": 0.7447,\n",
    "                            \"stats\": {\n",
    "                                \"count\": 2376,\n",
    "                                \"sum\": 1769.0,\n",
    "                                \"mean\": 0.7447,\n",
    "                                \"stderr\": 0.0089,\n",
    "                                \"min\": 0.0,\n",
    "                                \"max\": 1.0,\n",
    "                                \"variance\": 0.1902,\n",
    "                                \"stddev\": 0.4361\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"humaneval\": {\n",
    "            \"metrics\": {\n",
    "                \"pass_at_1\": {\n",
    "                    \"scores\": {\n",
    "                        \"pass_at_1\": {\n",
    "                            \"value\": 0.6451,\n",
    "                            \"stats\": {\n",
    "                                \"count\": 164,\n",
    "                                \"sum\": 105.8,\n",
    "                                \"mean\": 0.6451,\n",
    "                                \"stderr\": 0.0374,\n",
    "                                \"min\": 0.0,\n",
    "                                \"max\": 1.0,\n",
    "                                \"variance\": 0.229,\n",
    "                                \"stddev\": 0.4784\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"bleu\": {\n",
    "                    \"scores\": {\n",
    "                        \"bleu\": {\n",
    "                            \"value\": 0.1234,\n",
    "                            \"stats\": {\n",
    "                                \"count\": 164,\n",
    "                                \"sum\": 20.24,\n",
    "                                \"mean\": 0.1234,\n",
    "                                \"stderr\": 0.0156,\n",
    "                                \"min\": 0.0,\n",
    "                                \"max\": 1.0,\n",
    "                                \"variance\": 0.0399,\n",
    "                                \"stddev\": 0.1998\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"groups\": {\n",
    "        \"lm_evaluation_harness\": {\n",
    "            \"metrics\": {\n",
    "                \"avg_score\": {\n",
    "                    \"scores\": {\n",
    "                        \"avg_score\": {\n",
    "                            \"value\": 0.6993,\n",
    "                            \"stats\": {\n",
    "                                \"count\": 2,\n",
    "                                \"sum\": 1.3986,\n",
    "                                \"mean\": 0.6993,\n",
    "                                \"stderr\": 0.0542,\n",
    "                                \"min\": 0.6451,\n",
    "                                \"max\": 0.7534,\n",
    "                                \"variance\": 0.0058,\n",
    "                                \"stddev\": 0.0765\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n📄 Example NeMo Evaluator formatted result:\")\n",
    "print_json(example_nemo_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save NeMo formatted results to file\n",
    "def save_nemo_results(nemo_result, filename=\"nemo_evaluation_results.json\"):\n",
    "    \"\"\"Save NeMo Evaluator formatted results to JSON file.\"\"\"\n",
    "    import json\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(nemo_result, f, indent=2)\n",
    "\n",
    "    print(f\"💾 Results saved to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "# save_nemo_results(example_nemo_result, \"coding_reasoning_collection_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate NeMo Format Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate NeMo Evaluator format compatibility\n",
    "def validate_nemo_format(result_dict):\n",
    "    \"\"\"\n",
    "    Validate that the result dictionary conforms to NeMo Evaluator format.\n",
    "\n",
    "    Returns: (is_valid: bool, errors: list)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    # Check top-level structure\n",
    "    if not isinstance(result_dict, dict):\n",
    "        errors.append(\"Result must be a dictionary\")\n",
    "        return False, errors\n",
    "\n",
    "    if \"tasks\" not in result_dict:\n",
    "        errors.append(\"Missing required 'tasks' field\")\n",
    "\n",
    "    if \"groups\" not in result_dict:\n",
    "        errors.append(\"Missing required 'groups' field\")\n",
    "\n",
    "    # Validate tasks structure\n",
    "    if \"tasks\" in result_dict:\n",
    "        tasks = result_dict[\"tasks\"]\n",
    "        if not isinstance(tasks, dict):\n",
    "            errors.append(\"'tasks' must be a dictionary\")\n",
    "        else:\n",
    "            for task_name, task_data in tasks.items():\n",
    "                if not isinstance(task_data, dict):\n",
    "                    errors.append(f\"Task '{task_name}' must be a dictionary\")\n",
    "                    continue\n",
    "\n",
    "                if \"metrics\" not in task_data:\n",
    "                    errors.append(f\"Task '{task_name}' missing 'metrics' field\")\n",
    "                    continue\n",
    "\n",
    "                metrics = task_data[\"metrics\"]\n",
    "                for metric_name, metric_data in metrics.items():\n",
    "                    if \"scores\" not in metric_data:\n",
    "                        errors.append(f\"Metric '{metric_name}' in task '{task_name}' missing 'scores'\")\n",
    "                        continue\n",
    "\n",
    "                    scores = metric_data[\"scores\"]\n",
    "                    for score_name, score_data in scores.items():\n",
    "                        if \"value\" not in score_data:\n",
    "                            errors.append(f\"Score '{score_name}' missing 'value'\")\n",
    "                        if \"stats\" not in score_data:\n",
    "                            errors.append(f\"Score '{score_name}' missing 'stats'\")\n",
    "                        elif not isinstance(score_data[\"stats\"], dict):\n",
    "                            errors.append(f\"Score '{score_name}' stats must be a dictionary\")\n",
    "\n",
    "    is_valid = len(errors) == 0\n",
    "    return is_valid, errors\n",
    "\n",
    "# Validate the example result\n",
    "is_valid, validation_errors = validate_nemo_format(example_nemo_result)\n",
    "\n",
    "print(f\"🔍 NeMo format validation: {'✅ Valid' if is_valid else '❌ Invalid'}\")\n",
    "if validation_errors:\n",
    "    print(\"Validation errors:\")\n",
    "    for error in validation_errors:\n",
    "        print(f\"  - {error}\")\n",
    "else:\n",
    "    print(\"✅ All format requirements satisfied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated comprehensive usage of the Eval Hub API including:\n",
    "\n",
    "- ✅ **Basic Operations**: Health checks, provider/benchmark discovery\n",
    "- ✅ **Collection Management**: Create custom collections, list collections, and get detailed collection information\n",
    "- ✅ **Collection-Based Evaluations**: Execute evaluations using collections with automatic provider task aggregation\n",
    "- ✅ **Collection Results**: Monitor collection progress, retrieve aggregate results, and compare collection performance\n",
    "- ✅ **Model Management**: Register, list, update, and delete models\n",
    "- ✅ **Simple Evaluations**: Risk category-based evaluations\n",
    "- ✅ **Advanced Evaluations**: Explicit backend configuration\n",
    "- ✅ **NeMo Integration**: Single and multi-container setups\n",
    "- ✅ **Monitoring**: Status checking and progress tracking\n",
    "- ✅ **Management**: Cancellation and system metrics\n",
    "- ✅ **Error Handling**: Validation and error responses\n",
    "- ✅ **Batch Operations**: Multiple evaluation management\n",
    "- ✅ **Result Formatting**: NeMo Evaluator compatible result transformation and validation\n",
    "\n",
    "For production use, remember to:\n",
    "- Use proper API keys and authentication\n",
    "- Configure appropriate timeouts for your evaluation complexity\n",
    "- Monitor resource usage and system metrics\n",
    "- Handle errors gracefully in your applications\n",
    "- Use the async evaluation mode for long-running evaluations\n",
    "\n",
    "The Eval Hub provides a powerful and flexible API for orchestrating machine learning model evaluations across multiple backends and evaluation frameworks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
